# ğŸ’¬ é˜¶æ®µ 4ï¼šNLP å®æˆ˜é¡¹ç›®ä»£ç ï¼ˆBERT/GPT æ–¹å‘ï¼‰

# 1ï¸âƒ£ ä¸­æ–‡æƒ…æ„Ÿåˆ†æå™¨ï¼ˆç”µå•†è¯„è®º + BERTï¼‰
def chinese_sentiment_analysis():
    print("âœ… ä½¿ç”¨ BERT + Transformers å®ç°ä¸­æ–‡æƒ…æ„Ÿåˆ†æï¼š")
    print("1. å®‰è£…ä¾èµ–ï¼štransformers, datasets, torch")
    print("2. ç¤ºä¾‹æ¨¡å‹ï¼šuer/roberta-base-finetuned-dianping-chinese")
    print("3. æ¨ç†ç¤ºä¾‹ä»£ç ï¼š")
    print("""
from transformers import pipeline
classifier = pipeline("sentiment-analysis", model="uer/roberta-base-finetuned-dianping-chinese")
print(classifier("è¿™æ¬¾æ‰‹æœºæ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—è´­ä¹°ï¼"))
    """)

# 2ï¸âƒ£ æ–°é—»æ–‡æœ¬åˆ†ç±»å™¨ï¼ˆTHUCNewsï¼‰
def news_classification():
    print("âœ… ä½¿ç”¨ THUCNews è¯­æ–™ + BERT å¾®è°ƒè¿›è¡Œåˆ†ç±»")
    print("1. æ•°æ®é›†é“¾æ¥ï¼šhttps://thunlp.oss-cn-qingdao.aliyuncs.com/THUCNews.zip")
    print("2. ç¤ºä¾‹æ¨¡å‹ï¼šIDEA-CCNL/Erlangshen-Roberta-330M-Chinese")
    print("3. å¾®è°ƒå»ºè®®ä½¿ç”¨ transformers Trainer + datasets.load_dataset")

# 3ï¸âƒ£ FAQ é—®ç­”ç³»ç»Ÿï¼ˆEmbedding ç›¸ä¼¼åº¦åŒ¹é…ï¼‰
def faq_qa_system():
    from sentence_transformers import SentenceTransformer, util
    model = SentenceTransformer('shibing624/text2vec-base-chinese')
    faq = ["å¦‚ä½•è”ç³»å®¢æœï¼Ÿ", "å¦‚ä½•é€€è´§ï¼Ÿ", "å¦‚ä½•ä¿®æ”¹å¯†ç ï¼Ÿ"]
    faq_emb = model.encode(faq, convert_to_tensor=True)
    query = "æ€ä¹ˆè”ç³»å®¢æœå•Š"
    q_emb = model.encode(query, convert_to_tensor=True)
    scores = util.cos_sim(q_emb, faq_emb)
    best_idx = scores.argmax()
    print(f"åŒ¹é…åˆ°é—®é¢˜: {faq[best_idx]}")

# 4ï¸âƒ£ ç¿»è¯‘æ¨¡å‹å¾®è°ƒï¼ˆHelsinki NMTï¼‰
def translate_zh_en():
    from transformers import MarianMTModel, MarianTokenizer
    src_text = ["æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"]
    model_name = 'Helsinki-NLP/opus-mt-zh-en'
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(model_name)
    inputs = tokenizer(src_text, return_tensors="pt", padding=True)
    translated = model.generate(**inputs)
    print("ç¿»è¯‘ç»“æœ:", tokenizer.batch_decode(translated, skip_special_tokens=True))

# 5ï¸âƒ£ ç®€æ˜“ ChatBotï¼ˆTransformers + Promptï¼‰
def simple_chatbot():
    from transformers import AutoModelForCausalLM, AutoTokenizer
    model_name = "uer/gpt2-chinese-cluecorpussmall"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    while True:
        query = input("ä½ è¯´: ")
        if query.lower() in ['exit', 'quit']:
            break
        inputs = tokenizer.encode(query, return_tensors='pt')
        outputs = model.generate(inputs, max_length=50, pad_token_id=tokenizer.eos_token_id)
        print("æœºå™¨äºº:", tokenizer.decode(outputs[0], skip_special_tokens=True))

# ä¸»èœå•
if __name__ == '__main__':
    print("1. ä¸­æ–‡æƒ…æ„Ÿåˆ†æ\n2. æ–°é—»æ–‡æœ¬åˆ†ç±»\n3. FAQé—®ç­”ç³»ç»Ÿ\n4. ä¸­è‹±ç¿»è¯‘\n5. ChatBot èŠå¤©æœºå™¨äºº")
    choice = input("é€‰æ‹©é¡¹ç›®ç¼–å·ï¼š")
    if choice == '1': chinese_sentiment_analysis()
    elif choice == '2': news_classification()
    elif choice == '3': faq_qa_system()
    elif choice == '4': translate_zh_en()
    elif choice == '5': simple_chatbot()
