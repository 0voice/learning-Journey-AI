# ğŸ“· é˜¶æ®µ 3ï¼šè®¡ç®—æœºè§†è§‰å®æˆ˜é¡¹ç›®ä»£ç ç¤ºä¾‹

# 1ï¸âƒ£ YOLOv5 ç›®æ ‡æ£€æµ‹å®æˆ˜ï¼ˆéœ€å®‰è£… yolov5 ç¯å¢ƒï¼‰
def yolov5_detection():
    print("âœ… ä½¿ç”¨ yolov5 è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼š")
    print("1. å®‰è£…ä¾èµ–ï¼šgit clone https://github.com/ultralytics/yolov5 && pip install -r requirements.txt")
    print("2. æ•°æ®æ ‡æ³¨å·¥å…·ï¼šLabelImg / Roboflow / CVAT")
    print("3. å¼€å§‹è®­ç»ƒï¼špython train.py --img 640 --batch 16 --epochs 50 --data data.yaml --weights yolov5s.pt")
    print("4. æ¨ç†æ£€æµ‹ï¼špython detect.py --weights runs/train/exp/weights/best.pt --source your_image.jpg")

# 2ï¸âƒ£ å›¾åƒåˆ†å‰²ï¼ˆU-Netï¼šå¦‚é“è·¯åˆ†å‰²æˆ–åŒ»å­¦å›¾åƒï¼‰
import torch
import torch.nn as nn
class UNet(nn.Module):
    def __init__(self):
        super(UNet, self).__init__()
        def conv_block(in_c, out_c):
            return nn.Sequential(
                nn.Conv2d(in_c, out_c, 3, padding=1),
                nn.ReLU(),
                nn.Conv2d(out_c, out_c, 3, padding=1),
                nn.ReLU()
            )
        self.enc1 = conv_block(1, 64)
        self.pool = nn.MaxPool2d(2)
        self.dec1 = nn.ConvTranspose2d(64, 1, 2, stride=2)
        self.final = nn.Conv2d(1, 1, 1)

    def forward(self, x):
        x1 = self.enc1(x)
        x2 = self.pool(x1)
        x3 = self.dec1(x2)
        return torch.sigmoid(self.final(x3))

def unet_demo():
    model = UNet()
    dummy_input = torch.rand(1, 1, 128, 128)
    output = model(dummy_input)
    print("U-Net è¾“å‡ºå°ºå¯¸:", output.shape)

# 3ï¸âƒ£ OCR é¡¹ç›®ï¼šå›¾åƒè½¬æ–‡å­—è¯†åˆ«

def ocr_with_tesseract():
    import pytesseract
    from PIL import Image
    img = Image.open("sample_text_image.png")
    text = pytesseract.image_to_string(img, lang='eng')
    print("è¯†åˆ«ç»“æœï¼š\n", text)

# 4ï¸âƒ£ æ‘„åƒå¤´å®æ—¶äººè„¸è¯†åˆ«ä¸æ‰“å¡ç³»ç»Ÿ

def realtime_face_recognition():
    import cv2
    import face_recognition
    known_image = face_recognition.load_image_file("known.jpg")
    known_encoding = face_recognition.face_encodings(known_image)[0]

    cap = cv2.VideoCapture(0)
    while True:
        ret, frame = cap.read()
        rgb = frame[:, :, ::-1]
        faces = face_recognition.face_locations(rgb)
        encodings = face_recognition.face_encodings(rgb, faces)
        for (top, right, bottom, left), encoding in zip(faces, encodings):
            match = face_recognition.compare_faces([known_encoding], encoding)[0]
            name = "Known" if match else "Unknown"
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
            cv2.putText(frame, name, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)
        cv2.imshow("Face Recognition", frame)
        if cv2.waitKey(1) == 27:
            break
    cap.release()
    cv2.destroyAllWindows()

# 5ï¸âƒ£ AIç»˜ç”»ç”Ÿæˆå™¨ï¼ˆStable Diffusionï¼‰
def diffusion_art_generation():
    print("ğŸ–¼ï¸ ä½¿ç”¨ Diffusers åº“è¿è¡Œ Stable Diffusion")
    print("1. å®‰è£…ï¼špip install diffusers transformers accelerate")
    print("2. ç¤ºä¾‹ä»£ç ï¼š")
    print("""
from diffusers import StableDiffusionPipeline
import torch
pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
pipe = pipe.to("cuda")
image = pipe("a fantasy landscape, oil painting").images[0]
image.save("fantasy.png")
    """)

# ä¸»å‡½æ•°é€‰æ‹©è¿è¡Œæ¨¡å—
if __name__ == '__main__':
    print("é€‰æ‹©è®¡ç®—æœºè§†è§‰é¡¹ç›®ï¼š")
    print("1. YOLOv5 ç›®æ ‡æ£€æµ‹\n2. U-Net åˆ†å‰²\n3. OCR è¯†åˆ«\n4. å®æ—¶äººè„¸è¯†åˆ«\n5. Diffusion AIç»˜ç”»")
    choice = input("è¾“å…¥ç¼–å·ï¼š")
    if choice == '1': yolov5_detection()
    elif choice == '2': unet_demo()
    elif choice == '3': ocr_with_tesseract()
    elif choice == '4': realtime_face_recognition()
    elif choice == '5': diffusion_art_generation()
