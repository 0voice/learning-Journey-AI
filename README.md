# ğŸš€ğŸš€ğŸš€ AI Learning Hub Â· 2025 æœ€å¼º AI å­¦ä¹ è·¯çº¿ï¼Œä»å…¥é—¨åˆ°å®æˆ˜ï¼Œå…¨æµç¨‹è‡ªå­¦æŒ‡å—

---

<p align="center">
  <img src="Image1.png" alt="2025 AI Learning Banner" width="100%">
</p>

---

> **æŒæ¡ AIï¼Œä»è¿™é‡Œå¼€å§‹**  
> ä¸ºæ‰€æœ‰å¯¹ AI çŸ¥è¯†æ„Ÿå…´è¶£çš„å­¦ä¹ è€…æ‰“é€ çš„ AI / ML / DL ç³»ç»Ÿå­¦ä¹ è·¯çº¿ï¼Œæ¶µç›–ä¼˜è´¨è¯¾ç¨‹ã€ç»å…¸ä¹¦ç±ã€èåˆé¡¶çº§èµ„æºã€ä»£ç å®æˆ˜ä¸å¼€æºå·¥å…·ï¼Œä¸ºä½ æ‰“é€ ä»å…¥é—¨åˆ°å®æˆ˜å†åˆ°å‰æ²¿ç ”ç©¶çš„ AI æˆé•¿ä¹‹è·¯ã€‚  
>> **ç»“æ„åŒ– | æŒç»­æ›´æ–° | æœ€æ–°å­¦ä¹  |ç¤¾åŒºå…±å»º** [![GitHub Stars](https://img.shields.io/github/stars/0voice/learning-Journey-AI?style=social)](https://github.com/0voice/learning-Journey-AI)

---

ğŸ¯ **é€‚åˆå¯¹è±¡**ï¼š  
- æƒ³ä»é›¶èµ·æ­¥å­¦ä¹  AI çš„å¼€å‘è€…  
- éœ€è¦ä¸€æ¡ç³»ç»Ÿã€å¯è½åœ°çš„å­¦ä¹ è·¯å¾„çš„å­¦ä¹ è€…  
- å…³æ³¨è¡Œä¸šä¸€çº¿è¿›å±•ï¼Œæƒ³æŒæ¡å‰æ²¿æŠ€æœ¯çš„äºº

ğŸŒŸ **ä½ å°†è·å¾—**ï¼š  
âœ… æ¸…æ™°çš„é˜¶æ®µæ€§å­¦ä¹ è·¯çº¿å›¾  
âœ… ç²¾é€‰é«˜è´¨é‡å­¦ä¹ èµ„æºä¸å·¥å…·  
âœ… è¦†ç›–ä»åŸºç¡€åˆ°è¿›é˜¶çš„å®æˆ˜é¡¹ç›®  
âœ… å®šæœŸæ›´æ–°ï¼Œèšç„¦ä¸»æµä¸å‰æ²¿  
âœ… æ¬¢è¿å¼€æºç¤¾åŒºå…±åŒå»ºè®¾

> ğŸ’¡**ä¸å†ä¿¡æ¯è¿‡è½½ï¼Œä¸å†æ— ä»ä¸‹æ‰‹ï¼Œä»è¿™é‡Œå¼€å§‹ç³»ç»ŸæŒæ¡ AIã€‚**

---

## âœˆï¸ å­¦ä¹ è·¯çº¿å›¾ Overview

```mermaid
flowchart TD
    A[ç¼–ç¨‹åŸºç¡€Python] --> B[æ•°å­¦åŸºç¡€]
    B --> C[æœºå™¨å­¦ä¹ ]
    C --> D[æ·±åº¦å­¦ä¹ åŸºç¡€]
    D --> E[è®¡ç®—æœºè§†è§‰]
    D --> F[è‡ªç„¶è¯­è¨€å¤„ç†]
    D --> G[ç”Ÿæˆå¼AI]
    C --> H[å·¥å…·é“¾å®è·µ]
    H --> I[éƒ¨ç½²ä¸MLOps]
    E & F & G --> J[ä¸“é¡¹é¡¹ç›®å®æˆ˜]
    style A fill:#4CAF50,stroke:#388E3C
```

# ğŸ“š å­¦ä¹ è·¯å¾„åˆ†é˜¶æ®µ

---

# ğŸ“Œ [é˜¶æ®µ 0ï¼šå‰ç½®çŸ¥è¯†](https://github.com/0voice/learning-Journey-AI/tree/main/Python%20and%20Math) - [Pythonå…¥é—¨åŸºç¡€ï¼šé›¶åŸºç¡€å°ç™½å­¦ä¹ æŒ‡å—](Python%20and%20Math/python.md) 


### 1.å˜é‡ä¸æ•°æ®ç±»å‹
å˜é‡å°±åƒç”Ÿæ´»ä¸­çš„â€œæ ‡ç­¾â€ï¼Œç»™æ•°æ®èµ·åå­—æ–¹ä¾¿ä½¿ç”¨ï¼š
```python
# åˆ›å»ºå˜é‡
name = "å°æ˜"        # å­—ç¬¦ä¸² (æ–‡å­—)
age = 20             # æ•´æ•° (æ•°å­—)
height = 1.75        # æµ®ç‚¹æ•° (å¸¦å°æ•°ç‚¹çš„æ•°å­—)
is_student = True    # å¸ƒå°”å€¼ (çœŸ/å‡)

print(name)          # è¾“å‡º: å°æ˜
print(age + 5)       # è¾“å‡º: 25
```

### 2.æ§åˆ¶ç»“æ„ï¼šæ¡ä»¶åˆ¤æ–­
å¦‚æœ...é‚£ä¹ˆ...å¦åˆ™...çš„é€»è¾‘ï¼š
```python
# æ¡ä»¶åˆ¤æ–­ç¤ºä¾‹
temperature = 28

if temperature > 30:
    print("å¤ªçƒ­äº†ï¼å¼€ç©ºè°ƒ")
elif temperature > 20:
    print("å¤©æ°”çœŸèˆ’æœ")
else:
    print("æœ‰ç‚¹å†·ï¼Œå¤šç©¿ç‚¹")
```

### 3.æ§åˆ¶ç»“æ„ï¼šå¾ªç¯
é‡å¤æ‰§è¡ŒæŸäº›æ“ä½œï¼š
```python
# forå¾ªç¯ç¤ºä¾‹ - éå†åºåˆ—
fruits = ["è‹¹æœ", "é¦™è•‰", "æ©™å­"]

for fruit in fruits:
    print(f"æˆ‘çˆ±åƒ{fruit}")

# whileå¾ªç¯ç¤ºä¾‹ - è¾¾åˆ°æ¡ä»¶å‰é‡å¤
count = 0
while count < 5:
    print(f"è¿™æ˜¯ç¬¬{count+1}æ¬¡è¯´ä½ å¥½")
    count += 1
```

### 4. å‡½æ•°å®šä¹‰ä¸è°ƒç”¨
æŠŠå¸¸ç”¨æ“ä½œæ‰“åŒ…æˆ"å·¥å…·"ï¼š
```python
# å®šä¹‰å‡½æ•°ï¼šè®¡ç®—åœ†çš„é¢ç§¯
def circle_area(radius):
    area = 3.14 * radius * radius
    return area

# ä½¿ç”¨å‡½æ•°
print(circle_area(5))  # è®¡ç®—åŠå¾„ä¸º5çš„åœ†é¢ç§¯
```
### 5. ç±»ä¸é¢å‘å¯¹è±¡ç¼–ç¨‹
åˆ›å»ºè‡ªå®šä¹‰çš„æ•°æ®ç±»å‹ï¼š
```python
# å®šä¹‰"æ±½è½¦"ç±»
class Car:
    # åˆå§‹åŒ–æ–¹æ³•(ç»™æ–°è½¦è®¾ç½®å±æ€§)
    def __init__(self, brand, color):
        self.brand = brand
        self.color = color
    
    # ç±»çš„æ–¹æ³•(è¡Œä¸º)
    def drive(self):
        print(f"{self.color}è‰²çš„{self.brand}æ­£åœ¨è¡Œé©¶")

# ä½¿ç”¨ç±»åˆ›å»ºå¯¹è±¡
my_car = Car("ç‰¹æ–¯æ‹‰", "é»‘")
my_car.drive()  # è¾“å‡º: é»‘è‰²çš„ç‰¹æ–¯æ‹‰æ­£åœ¨è¡Œé©¶
```

### 6. å¼‚å¸¸å¤„ç†
é˜²æ­¢ç¨‹åºå‡ºé”™æ—¶å´©æºƒï¼š
```python
# å°è¯•æ‰“å¼€ä¸€ä¸ªä¸å­˜åœ¨çš„æ–‡ä»¶
try:
    file = open("ä¸å­˜åœ¨çš„æ–‡ä»¶.txt", "r")
except FileNotFoundError:
    print("æ‰¾ä¸åˆ°æ–‡ä»¶ï¼è¯·æ£€æŸ¥æ–‡ä»¶å")
```

## æ•°æ®ç»“æ„åŸºç¡€

### 1.åˆ—è¡¨/å…ƒç»„/å­—å…¸/é›†åˆ
| ç±»å‹   | ç‰¹ç‚¹                 | ç¤ºä¾‹                                 |
|--------|----------------------|--------------------------------------|
| åˆ—è¡¨   | å¯ä¿®æ”¹çš„æœ‰åºé›†åˆ     | `fruits = ["è‹¹æœ", "é¦™è•‰", "æ©™å­"]`  |
| å…ƒç»„   | ä¸å¯ä¿®æ”¹çš„æœ‰åºé›†åˆ   | `point = (3, 5)`                     |
| å­—å…¸   | é”®å€¼å¯¹é›†åˆ           | `student = {"å§“å": "å°æ˜", "å¹´é¾„": 20}` |
| é›†åˆ   | æ— é‡å¤å…ƒç´ çš„æ— åºé›†   | `unique_numbers = {1, 2, 3, 2} â†’ {1, 2, 3}` |
```python
# ç»¼åˆç¤ºä¾‹
# è´­ç‰©æ¸…å•ï¼ˆåˆ—è¡¨ï¼‰
shopping_list = ["ç‰›å¥¶", "é¸¡è›‹", "é¢åŒ…"]

# å•†å“ä»·æ ¼ï¼ˆå­—å…¸ï¼‰
prices = {
    "ç‰›å¥¶": 15.5,
    "é¸¡è›‹": 12.8,
    "é¢åŒ…": 8.0
}

# è®¡ç®—æ€»ä»·
total = 0
for item in shopping_list:
    if item in prices:
        total += prices[item]

print(f"è´­ç‰©æ€»ä»·: {total}å…ƒ")  # è¾“å‡º: è´­ç‰©æ€»ä»·: 36.3å…ƒ
```
### 2.æ ˆä¸é˜Ÿåˆ—
ä¸¤ç§æ•°æ®æ“ä½œæ–¹å¼ï¼š
- â€‹â€‹æ ˆï¼ˆStackï¼‰â€‹â€‹ï¼šåè¿›å…ˆå‡ºï¼ˆLIFOï¼‰ï¼Œåƒå ç›˜å­
```python
# ä½¿ç”¨åˆ—è¡¨å®ç°æ ˆ
stack = []
stack.append("ç¬¬1ç›˜")  # æ”¾å…¥
stack.append("ç¬¬2ç›˜")
top = stack.pop()      # å–å‡º: "ç¬¬2ç›˜"
```
- â€‹â€‹é˜Ÿåˆ—ï¼ˆQueueï¼‰â€‹â€‹ï¼šå…ˆè¿›å…ˆå‡ºï¼ˆFIFOï¼‰ï¼Œåƒæ’é˜Ÿ
```python
# ä½¿ç”¨é˜Ÿåˆ—
from collections import deque
queue = deque()
queue.append("ç¬¬1äºº")  # æ’é˜Ÿ
queue.append("ç¬¬2äºº")
first = queue.popleft()  # æœåŠ¡: "ç¬¬1äºº"
```

### 3. é“¾è¡¨/æ ‘/å›¾
å¸¸ç”¨æ•°æ®ç»“æ„å¯è§†åŒ–æ¯”è¾ƒï¼š
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
graph TD
    A[æ•°æ®ç»“æ„] --> B[çº¿æ€§]
    A --> C[éçº¿æ€§]
    
    B --> D[é“¾è¡¨]
    D --> D1[å•å‘é“¾è¡¨]
    D --> D2[åŒå‘é“¾è¡¨]
    
    C --> E[æ ‘]
    E --> E1[äºŒå‰æ ‘]
    E --> E2[å¹³è¡¡æ ‘]
    
    C --> F[å›¾]
    F --> F1[æœ‰å‘å›¾]
    F --> F2[æ— å‘å›¾]
```
å®é™…åº”ç”¨ï¼š
- â€‹â€‹é“¾è¡¨â€‹â€‹ï¼šæµè§ˆå™¨å†å²è®°å½•
- æ ‘â€‹â€‹ï¼šæ–‡ä»¶ç³»ç»Ÿç»„ç»‡
- å›¾â€‹â€‹ï¼šç¤¾äº¤ç½‘ç»œå…³ç³»

### 4. æ—¶é—´/ç©ºé—´å¤æ‚åº¦åˆ†æ
è¯„ä¼°ç®—æ³•æ•ˆç‡çš„æ–¹æ³•ï¼š
- æ—¶é—´å¤æ‚åº¦â€‹â€‹ï¼šç®—æ³•è¿è¡Œæ—¶é—´éšè¾“å…¥è§„æ¨¡å¢é•¿çš„å˜åŒ–
- ç©ºé—´å¤æ‚åº¦â€‹â€‹ï¼šç®—æ³•è¿è¡Œæ‰€éœ€å†…å­˜ç©ºé—´çš„å˜åŒ–

å¸¸è§æ—¶é—´å¤æ‚åº¦ï¼š
- O(1) - å›ºå®šæ—¶é—´ï¼ˆæœ€å¥½ï¼‰
- O(log n) - å¯¹æ•°æ—¶é—´ï¼ˆå¾ˆå¥½ï¼‰
- O(n) - çº¿æ€§æ—¶é—´ï¼ˆå¥½ï¼‰
- O(nÂ²) - å¹³æ–¹æ—¶é—´ï¼ˆè¾ƒå·®ï¼‰

ç¤ºä¾‹ï¼šæŸ¥æ‰¾åˆ—è¡¨ä¸­æ˜¯å¦å­˜åœ¨æŸå…ƒç´ 
```python
# ç®€å•æŸ¥æ‰¾ - O(n)
def simple_search(items, target):
    for item in items:
        if item == target:
            return True
    return False

# äºŒåˆ†æŸ¥æ‰¾ï¼ˆæœ‰åºåˆ—è¡¨ï¼‰- O(log n)
def binary_search(items, target):
    low, high = 0, len(items)-1
    while low <= high:
        mid = (low + high) // 2
        if items[mid] == target:
            return True
        elif items[mid] < target:
            low = mid + 1
        else:
            high = mid - 1
    return False
```

# ç®—æ³•åŸºç¡€
## 1. æœç´¢ç®—æ³•
åœ¨æ•°æ®é›†ä¸­æŸ¥æ‰¾ç‰¹å®šå…ƒç´ ï¼š
| æ–¹æ³•       | åœºæ™¯         | ä¼˜ç‚¹         | ç¼ºç‚¹               |
|------------|--------------|--------------|--------------------|
| çº¿æ€§æœç´¢   | æ— åºåˆ—è¡¨     | ç®€å•ç›´æ¥     | æ•ˆç‡ä½(O(n))      |
| äºŒåˆ†æœç´¢   | æœ‰åºåˆ—è¡¨     | é«˜æ•ˆ(O(log n)) | è¦æ±‚åˆ—è¡¨æœ‰åº       |

ç¤ºä¾‹ï¼šäºŒåˆ†æŸ¥æ‰¾å®ç°
```python
def binary_search(items, target):
    # èµ·ç‚¹å’Œç»ˆç‚¹ç´¢å¼•
    low, high = 0, len(items)-1
    
    while low <= high:
        # è®¡ç®—ä¸­é—´ä½ç½®
        mid = (low + high) // 2
        mid_value = items[mid]
        
        # æ‰¾åˆ°ç›®æ ‡
        if mid_value == target:
            return mid
        
        # ç›®æ ‡åœ¨å³ä¾§
        elif mid_value < target:
            low = mid + 1
        
        # ç›®æ ‡åœ¨å·¦ä¾§
        else:
            high = mid - 1
    
    # æœªæ‰¾åˆ°
    return -1
```
## 2. æ’åºç®—æ³•
é‡æ–°æ’åˆ—å…ƒç´ é¡ºåºï¼š
| æ–¹æ³•       | å¹³å‡å¤æ‚åº¦       | ç‰¹ç‚¹              |
|------------|------------------|-------------------|
| å†’æ³¡æ’åº   | \( O(n^2) \)     | ç®€å•ä½†æ…¢          |
| å¿«é€Ÿæ’åº   | \( O(n \log n) \) | é«˜æ•ˆï¼Œå¸¸ç”¨        |
| å½’å¹¶æ’åº   | \( O(n \log n) \) | ç¨³å®šï¼Œå¤§æ•°æ®å¤„ç†  |

å¿«é€Ÿæ’åºç¤ºä¾‹ï¼š
```python
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    
    pivot = arr[len(arr) // 2]  # é€‰æ‹©ä¸­é—´å€¼ä½œä¸ºåŸºå‡†
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    
    return quicksort(left) + middle + quicksort(right)
```
## 3. åŠ¨æ€è§„åˆ’
æŠŠå¤§é—®é¢˜åˆ†è§£æˆå°é—®é¢˜ï¼Œå¹¶å­˜å‚¨å°é—®é¢˜ç»“æœï¼š
- é€‚åˆæ±‚è§£ï¼šæ–æ³¢é‚£å¥‘æ•°åˆ—ã€æœ€çŸ­è·¯å¾„ç­‰
- æ ¸å¿ƒæ€æƒ³ï¼šé¿å…é‡å¤è®¡ç®—ï¼Œä½¿ç”¨ç¼“å­˜
æ–æ³¢é‚£å¥‘æ•°åˆ—åŠ¨æ€è§„åˆ’å®ç°ï¼š
```python
def fib(n):
    # å­˜å‚¨è®¡ç®—ç»“æœ
    cache = [0, 1]  
    
    # ä»2å¼€å§‹è®¡ç®—å¹¶å­˜å‚¨ç»“æœ
    for i in range(2, n+1):
        cache.append(cache[i-1] + cache[i-2])
    
    return cache[n]

print(fib(10))  # è¾“å‡º: 55
```
## 4. è´ªå¿ƒç®—æ³•
æ¯ä¸€æ­¥éƒ½é€‰æ‹©å½“å‰æœ€ä¼˜è§£ï¼š
- ç‰¹ç‚¹ï¼šç®€å•é«˜æ•ˆï¼Œä½†ä¸ä¸€å®šèƒ½å¾—åˆ°å…¨å±€æœ€ä¼˜
- åº”ç”¨åœºæ™¯ï¼šé›¶é’±å…‘æ¢ã€å“ˆå¤«æ›¼ç¼–ç ç­‰
é›¶é’±å…‘æ¢ç¤ºä¾‹ï¼š
```python
def coin_change(coins, amount):
    # æ’åºç¡¬å¸ï¼ˆä»å¤§åˆ°å°ï¼‰
    coins.sort(reverse=True)
    result = []
    
    # å°è¯•ä½¿ç”¨æ¯ä¸ªç¡¬å¸
    for coin in coins:
        while amount >= coin:
            amount -= coin
            result.append(coin)
    
    return result

# ç”¨æœ€å°‘ç¡¬å¸ç»„æˆ86åˆ†
coins = [1, 5, 10, 25]
print(coin_change(coins, 86))  # [25, 25, 25, 10, 1]
```

# Git/GitHub ç‰ˆæœ¬æ§åˆ¶
## 1. ç‰ˆæœ¬æ§åˆ¶åŸºç¡€
ä»€ä¹ˆæ˜¯ç‰ˆæœ¬æ§åˆ¶ï¼Ÿè®°å½•æ–‡ä»¶å˜åŒ–çš„å†å²è®°å½•ç³»ç»Ÿ
æ ¸å¿ƒæ¦‚å¿µï¼š
- ä»“åº“ï¼ˆRepositoryï¼‰â€‹â€‹ï¼šé¡¹ç›®çš„æ–‡ä»¶å¤¹åŠå…¶å†å²è®°å½•
- æäº¤ï¼ˆCommitï¼‰â€‹â€‹ï¼šä¸€æ¬¡ç‰ˆæœ¬ä¿å­˜ï¼ˆå«æè¿°ä¿¡æ¯ï¼‰
- åˆ†æ”¯ï¼ˆBranchï¼‰â€‹â€‹ï¼šéš”ç¦»çš„å®éªŒç©ºé—´
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
graph LR
    A[å¼€å§‹] --> B[ä¿®æ”¹æ–‡ä»¶]
    B --> C[æ·»åŠ å˜æ›´åˆ°æš‚å­˜åŒº]
    C --> D[åˆ›å»ºæäº¤]
    D --> E[æ¨é€åˆ°è¿œç¨‹ä»“åº“]
```

## 2. åˆ†æ”¯ç®¡ç†
åœ¨ä¸åŒåˆ†æ”¯ä¸Šè¿›è¡Œå¼€å‘ï¼š
```bash
# 1. åˆ›å»ºæ–°åˆ†æ”¯
git branch new-feature

# 2. åˆ‡æ¢åˆ°è¯¥åˆ†æ”¯
git checkout new-feature

# 3. åœ¨æ–°åˆ†æ”¯ä¸Šè¿›è¡Œå¼€å‘ä¿®æ”¹...
git add .
git commit -m "æ·»åŠ æ–°åŠŸèƒ½"

# 4. å®Œæˆååˆå¹¶åˆ°ä¸»åˆ†æ”¯
git checkout main
git merge new-feature

# 5. æ¨é€åˆ°è¿œç¨‹ä»“åº“
git push origin main
```
## 3. åˆå¹¶è¯·æ±‚å·¥ä½œæµï¼ˆPull Requestï¼‰
å›¢é˜Ÿåä½œçš„æ ‡å‡†æµç¨‹ï¼š
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
sequenceDiagram
    participant A as å¼€å‘è€…
    participant B as GitHub
    participant C as å›¢é˜Ÿé¢†å¯¼
    
    A->>B: 1. æ¨é€ç‰¹æ€§åˆ†æ”¯
    A->>B: 2. åˆ›å»ºPull Request
    C->>B: 3. å®¡æŸ¥ä»£ç 
    C->>B: 4. æ‰¹å‡†è¯·æ±‚
    B->>B: 5. è‡ªåŠ¨åˆå¹¶ä»£ç 
```
## 4. ä»£ç åä½œæœ€ä½³å®è·µ
1.â€‹â€‹æ¯æ—¥æäº¤â€‹â€‹ï¼šå°æ­¥å‰è¿›ï¼Œå¤šæ¬¡æäº¤  
2.â€‹â€‹æ¸…æ™°çš„æäº¤ä¿¡æ¯â€‹â€‹ï¼š
```bash
# å·®çš„ä¿¡æ¯: "ä¿®å¤é—®é¢˜"
# å¥½çš„ä¿¡æ¯: "ä¿®å¤ç™»å½•é¡µé¢éªŒè¯ç ä¸æ˜¾ç¤ºçš„é—®é¢˜"
```
â€‹â€‹3.åˆ†æ”¯å‘½åè§„èŒƒâ€‹â€‹ï¼š
- feature/user-authenticationï¼ˆæ–°åŠŸèƒ½ï¼‰
- fix/button-alignmentï¼ˆä¿®å¤é—®é¢˜ï¼‰  

4.ä½¿ç”¨.gitignoreæ–‡ä»¶æ’é™¤ä¸éœ€è¦è·Ÿè¸ªçš„æ–‡ä»¶  

5.å®šæœŸgit pullæ‹‰å–ä»–äººæ›´æ”¹ï¼Œå‡å°‘å†²çª

# [æ•°å­¦åŸºç¡€å…¥é—¨ï¼šå°ç™½ä¹Ÿèƒ½æ‡‚çš„AIæ•°å­¦](Python-and-Math/math.md)

## çº¿æ€§ä»£æ•° - æ•°æ®çš„åŸºæœ¬éª¨æ¶
### çŸ©é˜µè¿ç®—ï¼šæ•°æ®çš„è¡¨æ ¼
çŸ©é˜µå°±åƒExcelè¡¨æ ¼ï¼Œç”¨æ¥ç»„ç»‡æ•°å­—ï¼š
```python
import numpy as np

# åˆ›å»º2x2çŸ©é˜µ
matrix = np.array([[1, 2], 
                   [3, 4]])
                   
# çŸ©é˜µåŠ æ³•
matrix + 2  # æ‰€æœ‰å…ƒç´ åŠ 2 â†’ [[3,4],[5,6]]

# çŸ©é˜µä¹˜æ³•
np.dot(matrix, matrix)  # çŸ©é˜µè‡ªä¹˜ â†’ [[7,10],[15,22]]
```
### å‘é‡ç©ºé—´ï¼šç®­å¤´æŒ‡å‘çš„æ–¹å‘
å‘é‡å°±åƒå¸¦æ–¹å‘çš„ç®­å¤´ï¼š
```python
# åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„ä¸¤ä¸ªå‘é‡
vector_a = np.array([1, 2, 3])
vector_b = np.array([4, 5, 6])

# å‘é‡çš„ç‚¹ç§¯ï¼ˆæŠ•å½±ï¼‰
dot_product = np.dot(vector_a, vector_b)  # 1Ã—4 + 2Ã—5 + 3Ã—6 = 32

# å‘é‡é•¿åº¦
length_a = np.linalg.norm(vector_a)  # âˆš(1Â²+2Â²+3Â²) â‰ˆ 3.74
```
### ç‰¹å¾å€¼/ç‰¹å¾å‘é‡ï¼šçŸ©é˜µçš„æœ¬è´¨
å½“çŸ©é˜µä½œç”¨åœ¨ç‰¹å®šå‘é‡ä¸Šæ—¶ä¸æ”¹å˜æ–¹å‘ï¼š
```python
# æ±‚çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
matrix = np.array([[2, 1],
                   [1, 2]])
                   
eigenvalues, eigenvectors = np.linalg.eig(matrix)

print("ç‰¹å¾å€¼:", eigenvalues)    # [3., 1.]
print("ç‰¹å¾å‘é‡:\n", eigenvectors)  # [[ 0.707, -0.707], [0.707, 0.707]]
```
### å¥‡å¼‚å€¼åˆ†è§£(SVD)ï¼šæ•°æ®çš„æœ¬è´¨æ‹†åˆ†
å°†ä»»æ„çŸ©é˜µåˆ†è§£ä¸ºä¸‰ä¸ªç‰¹æ®ŠçŸ©é˜µç›¸ä¹˜ï¼š
```python
# å›¾åƒå‹ç¼©ç¤ºä¾‹ï¼ˆå®é™…åº”ç”¨ä¸­ï¼‰
from skimage import data
from skimage.transform import resize
import matplotlib.pyplot as plt

# åŠ è½½å°å›¾åƒ
image = resize(data.astronaut(), (100, 100))
gray_image = np.mean(image, axis=2)

# è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£
U, s, VT = np.linalg.svd(gray_image, full_matrices=False)

# ä»…ä¿ç•™å‰20ä¸ªç‰¹å¾é‡å»ºå›¾åƒ
k = 20
reconstructed = U[:, :k] @ np.diag(s[:k]) @ VT[:k, :]

# æ˜¾ç¤ºå‹ç¼©å‰åå¯¹æ¯”
fig, (ax1, ax2) = plt.subplots(1, 2)
ax1.imshow(gray_image, cmap='gray')
ax1.set_title('åŸå§‹å›¾åƒ')
ax2.imshow(reconstructed, cmap='gray')
ax2.set_title('å‹ç¼©åå›¾åƒ (SVD)')
plt.show()
```
## æ¦‚ç‡ç»Ÿè®¡ - é¢„æµ‹ä¸ä¸ç¡®å®šæ€§çš„è‰ºæœ¯
### æ¦‚ç‡åˆ†å¸ƒï¼šäº‹ä»¶å‘ç”Ÿçš„å¯èƒ½æ€§
```python
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm, binom, poisson

# æ­£æ€åˆ†å¸ƒï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰
x = np.linspace(-5, 5, 100)
plt.plot(x, norm.pdf(x, 0, 1), label='æ­£æ€åˆ†å¸ƒ')

# äºŒé¡¹åˆ†å¸ƒï¼ˆæŠ›ç¡¬å¸ï¼‰
n, p = 10, 0.5
x_binom = np.arange(0, 11)
plt.stem(x_binom, binom.pmf(x_binom, n, p), 'bo', label='äºŒé¡¹åˆ†å¸ƒ')

# æ³Šæ¾åˆ†å¸ƒï¼ˆç½•è§äº‹ä»¶ï¼‰
lambda_ = 3
x_poisson = np.arange(0, 10)
plt.stem(x_poisson, poisson.pmf(x_poisson, lambda_), 'g^', label='æ³Šæ¾åˆ†å¸ƒ')

plt.legend()
plt.title('å¸¸è§æ¦‚ç‡åˆ†å¸ƒ')
plt.xlabel('æ•°å€¼')
plt.ylabel('æ¦‚ç‡å¯†åº¦')
plt.show()
```
### è´å¶æ–¯å®šç†ï¼šæ–°è¯æ®æ›´æ–°ä¿¡å¿µ
**åŒ»ç”Ÿè¯Šæ–­ç–¾ç—…çš„æƒ…æ™¯ï¼šâ€‹**
- å‡è®¾ï¼š
+ ç–¾ç—…Dæ‚£ç—…ç‡: 1% â†’ P(D) = 0.01
+ æ£€æµ‹çµæ•åº¦: 99% â†’ P(é˜³æ€§|D) = 0.99
+ æ£€æµ‹ç‰¹å¼‚åº¦: 95% â†’ P(é˜´æ€§|å¥åº·) = 0.95
æ±‚P(ç¡®å®æœ‰ç—…|æ£€æµ‹é˜³æ€§)?
```python
# è®¡ç®—è´å¶æ–¯æ¦‚ç‡
p_disease = 0.01      # P(D)
p_positive_given_disease = 0.99  # P(é˜³æ€§|D)
p_negative_given_healthy = 0.95  # P(é˜´æ€§|å¥åº·)

# P(é˜³æ€§|å¥åº·) = 1 - P(é˜´æ€§|å¥åº·)
p_positive_given_healthy = 1 - p_negative_given_healthy

# P(é˜³æ€§) = P(é˜³æ€§|D) * P(D) + P(é˜³æ€§|å¥åº·) * P(å¥åº·)
p_positive = (p_positive_given_disease * p_disease) + (p_positive_given_healthy * (1-p_disease))

# P(D|é˜³æ€§) = [P(é˜³æ€§|D) * P(D)] / P(é˜³æ€§)
p_disease_given_positive = (p_positive_given_disease * p_disease) / p_positive

print(f"æ£€æµ‹é˜³æ€§åçœŸæ­£æ‚£ç—…çš„æ¦‚ç‡: {p_disease_given_positive*100:.2f}%")  # â‰ˆ16.2%
```
### å‡è®¾æ£€éªŒï¼šåˆ¤æ–­å·®å¼‚æ˜¯å¦çœŸå®
**â€‹â€‹å­¦ç”ŸAå’ŒBè°æˆç»©æ›´å¥½**â€‹â€‹
+ Aç­å¹³å‡åˆ†ï¼š78åˆ†ï¼ˆ30äººï¼‰
+ Bç­å¹³å‡åˆ†ï¼š82åˆ†ï¼ˆ30äººï¼‰
+ å·®å¼‚æ˜¾è‘—å—ï¼Ÿ
```python
from scipy import stats

# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆæ–¹å·®ä¸º10ï¼‰
np.random.seed(42)
class_a = np.random.normal(78, 10, 30)
class_b = np.random.normal(82, 10, 30)

# è¿›è¡Œtæ£€éªŒ
t_stat, p_value = stats.ttest_ind(class_a, class_b)

alpha = 0.05  # æ˜¾è‘—æ€§æ°´å¹³
if p_value < alpha:
    print(f"på€¼ = {p_value:.4f} < 0.05ï¼Œä¸¤ç»„æœ‰æ˜¾è‘—å·®å¼‚")
else:
    print(f"på€¼ = {p_value:.4f} >= 0.05ï¼Œä¸¤ç»„æ— æ˜¾è‘—å·®å¼‚")
```
### å›å½’åˆ†æï¼šé¢„æµ‹è¶‹åŠ¿
æ ¹æ®æˆ¿å±‹é¢ç§¯é¢„æµ‹ä»·æ ¼ï¼š
```python
from sklearn.linear_model import LinearRegression

# æ ·æœ¬æ•°æ®ï¼ˆé¢ç§¯ vs ä»·æ ¼ï¼‰
areas = np.array([50, 70, 90, 110, 130]).reshape(-1, 1)  # mÂ²
prices = np.array([200, 240, 290, 340, 380])  # ä¸‡å…ƒ

# åˆ›å»ºæ¨¡å‹å¹¶æ‹Ÿåˆ
model = LinearRegression()
model.fit(areas, prices)

# é¢„æµ‹80å¹³ç±³æˆ¿å­çš„ä»·æ ¼
prediction = model.predict([[80]])
print(f"é¢„æµ‹80å¹³ç±³æˆ¿å±‹ä»·æ ¼ï¼š{prediction[0]:.1f}ä¸‡å…ƒ")

# ç»˜åˆ¶æ•°æ®ç‚¹åŠæ‹Ÿåˆçº¿
plt.scatter(areas, prices, label='å®é™…ä»·æ ¼')
plt.plot(areas, model.predict(areas), 'r-', label='é¢„æµ‹è¶‹åŠ¿')
plt.scatter([80], prediction, c='g', marker='*', s=200, label='é¢„æµ‹ç‚¹')
plt.xlabel('é¢ç§¯(mÂ²)')
plt.ylabel('ä»·æ ¼(ä¸‡å…ƒ)')
plt.legend()
plt.show()
```
# å¾®ç§¯åˆ† - å˜åŒ–çš„æ•°å­¦è¯­è¨€
## å¯¼æ•°ä¸ç§¯åˆ†ï¼šå˜åŒ–ä¸ç´¯ç§¯  
**â€‹â€‹å¯¼æ•° â‰ˆ ç¬æ—¶é€Ÿåº¦ï¼Œç§¯åˆ† â‰ˆ æ€»è·ç¦»â€‹**
```python
# æŸè½¦è¾†çš„è¿åŠ¨å‡½æ•°ï¼šä½ç½® = æ—¶é—´Â²
t = np.linspace(0, 5, 100)  # 0åˆ°5ç§’
position = t**2              # ä½ç½®å‡½æ•°

# è®¡ç®—å¯¼æ•°ï¼ˆé€Ÿåº¦ï¼‰
# å¯¼æ•°çš„æ•°å€¼è®¡ç®—ï¼šdy/dx â‰ˆ Î”y/Î”x
velocity = np.gradient(position, t)  # 2t

# è®¡ç®—ç§¯åˆ†ï¼ˆæ€»è·¯ç¨‹ï¼‰
# ç§¯åˆ†çš„æ•°å€¼è®¡ç®—ï¼ˆç´¯åŠ ï¼‰
distance = np.cumsum(velocity * np.diff(t, prepend=0))

# ç»˜åˆ¶ç»“æœ
plt.figure(figsize=(10, 6))
plt.subplot(211)
plt.plot(t, position, 'b-', label='ä½ç½®')
plt.plot(t, velocity, 'g--', label='é€Ÿåº¦(å¯¼æ•°)')
plt.legend()
plt.title('ä½ç½®ä¸é€Ÿåº¦å…³ç³»')

plt.subplot(212)
plt.plot(t, distance, 'r-', label='è·¯ç¨‹(ç§¯åˆ†)')
plt.legend()
plt.xlabel('æ—¶é—´(ç§’)')
plt.show()
```
### åå¯¼æ•°ï¼šå¤šç»´ç©ºé—´çš„å˜åŒ–ç‡
æ¸©åº¦åœºçš„å˜åŒ–ï¼ˆéšæ—¶é—´+ä½ç½®ï¼‰ï¼š
```python
from mpl_toolkits.mplot3d import Axes3D

# åˆ›å»ºæ—¶é—´å’Œç©ºé—´çš„ç½‘æ ¼
x = np.linspace(0, 10, 100)  # ç©ºé—´åæ ‡
t = np.linspace(0, 5, 100)    # æ—¶é—´åæ ‡
X, T = np.meshgrid(x, t)

# æ¸©åº¦å‡½æ•°ï¼šæ¸©åº¦ = e^{-0.1t} * sin(x)
Z = np.exp(-0.1*T) * np.sin(X)

# ç»˜åˆ¶3Dæ¸©åº¦åœº
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, T, Z, cmap='viridis')
ax.set_xlabel('ä½ç½®(x)')
ax.set_ylabel('æ—¶é—´(t)')
ax.set_zlabel('æ¸©åº¦(â„ƒ)')
ax.set_title('ç©ºé—´æ¸©åº¦åˆ†å¸ƒéšæ—¶é—´å˜åŒ–')
plt.show()
```
### æ¢¯åº¦ï¼šæœ€é™¡çš„ä¸Šå±±æ–¹å‘
```python
# å®šä¹‰ä¸€ä¸ªå±±å³°å½¢çŠ¶çš„å‡½æ•°
def mountain(x, y):
    return np.exp(-0.1*(x**2 + y**2)) * np.cos(0.5*x)

# åˆ›å»ºç½‘æ ¼
x = np.linspace(-3, 3, 100)
y = np.linspace(-3, 3, 100)
X, Y = np.meshgrid(x, y)
Z = mountain(X, Y)

# è®¡ç®—æ¢¯åº¦ï¼ˆä¸‹å±±æ–¹å‘ï¼‰
gy, gx = np.gradient(Z)
skip = 5  # æ˜¾ç¤ºéƒ¨åˆ†ç®­å¤´

# ç»˜åˆ¶ç­‰é«˜çº¿å›¾
plt.figure(figsize=(10, 8))
plt.contourf(X, Y, Z, 20, cmap='viridis')
plt.colorbar()
plt.quiver(X[::skip, ::skip], Y[::skip, ::skip], 
           -gx[::skip, ::skip], -gy[::skip, ::skip], 
           scale=50, color='white')  # è´Ÿæ¢¯åº¦è¡¨ç¤ºæœ€é™¡ä¸‹é™æ–¹å‘
plt.title('åœ°å½¢æ¢¯åº¦å›¾ - ç™½è‰²ç®­å¤´æŒ‡å‘æœ€é™¡ä¸‹é™æ–¹å‘')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```
### æ³°å‹’çº§æ•°ï¼šç”¨å¤šé¡¹å¼é€¼è¿‘å¤æ‚å‡½æ•°
ç”¨å¤šé¡¹å¼é€¼è¿‘æ­£å¼¦å‡½æ•°ï¼š
```python
# æ­£å¼¦å‡½æ•°åŠå…¶æ³°å‹’å±•å¼€
x = np.linspace(-10, 10, 500)
sin_x = np.sin(x)

# ä¸åŒé˜¶æ•°çš„æ³°å‹’å±•å¼€
taylor1 = x  # 1é˜¶
taylor3 = x - x**3/6  # 3é˜¶
taylor5 = taylor3 + x**5/120  # 5é˜¶

# ç»˜åˆ¶æ¯”è¾ƒå›¾
plt.figure(figsize=(10, 6))
plt.plot(x, sin_x, 'b-', lw=3, label='çœŸå® sin(x)')
plt.plot(x, taylor1, 'g--', label='1é˜¶å±•å¼€')
plt.plot(x, taylor3, 'r-.', label='3é˜¶å±•å¼€')
plt.plot(x, taylor5, 'm:', lw=2, label='5é˜¶å±•å¼€')
plt.ylim(-3, 3)
plt.legend()
plt.title('æ³°å‹’çº§æ•°é€¼è¿‘æ­£å¼¦å‡½æ•°')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid(True)
plt.show()
```
# ä¼˜åŒ–ç†è®º - å¯»æ‰¾æœ€ä½³è§£å†³æ–¹æ¡ˆ
## æ¢¯åº¦ä¸‹é™ï¼šä¸€æ­¥ä¸€æ­¥æ‰¾åˆ°æœ€ä½ç‚¹
### å¯»æ‰¾å‡½æ•°æœ€ä½ç‚¹ï¼š
```python
# å®šä¹‰å‡½æ•°ï¼šf(x) = x^4 - 3x^3 + 2
def f(x):
    return x**4 - 3*x**3 + 2

# å¯¼æ•°ï¼šf'(x) = 4x^3 - 9x^2
def df(x):
    return 4*x**3 - 9*x**2

# æ¢¯åº¦ä¸‹é™
x = 2.0     # åˆå§‹ç‚¹
lr = 0.01   # å­¦ä¹ ç‡
steps = 50  # è¿­ä»£æ¬¡æ•°

# è®°å½•è·¯å¾„
path = [x]

for i in range(steps):
    grad = df(x)
    x = x - lr * grad  # å‘ä¸‹èµ°ä¸€æ­¥
    path.append(x)
    
# ç»˜åˆ¶å‡½æ•°åŠä¸‹é™è·¯å¾„
x_vals = np.linspace(-1, 3, 200)
plt.plot(x_vals, f(x_vals), 'b-', lw=2, label='f(x)')
plt.scatter(path, f(np.array(path)), c='r', marker='o')
for i in range(1, len(path)):
    plt.annotate('', xy=(path[i], f(path[i])), 
                xytext=(path[i-1], f(path[i-1])),
                arrowprops=dict(arrowstyle='->', color='r'))
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('æ¢¯åº¦ä¸‹é™è¿‡ç¨‹')
plt.grid(True)
plt.show()
```
### çº¦æŸä¼˜åŒ–ï¼šå¸¦é™åˆ¶çš„æœ€ä¼˜åŒ–é—®é¢˜
```python
from scipy.optimize import minimize

# ç›®æ ‡å‡½æ•°ï¼šf(x,y) = (x-1)Â² + (y-2.5)Â²
objective = lambda x: (x[0]-1)**2 + (x[1]-2.5)**2

# çº¦æŸæ¡ä»¶ï¼š
# x - 2y >= -1    â†’ çº¦æŸ1
# -x - 2y >= -6   â†’ çº¦æŸ2
# -x + 2y >= -2   â†’ çº¦æŸ3
constraints = [
    {'type': 'ineq', 'fun': lambda x: x[0] - 2*x[1] + 1},  # â‰¥0
    {'type': 'ineq', 'fun': lambda x: -x[0] - 2*x[1] + 6},
    {'type': 'ineq', 'fun': lambda x: -x[0] + 2*x[1] + 2}
]

# åˆå§‹çŒœæµ‹
x0 = [0, 0]

# æ±‚è§£
solution = minimize(objective, x0, constraints=constraints)
print(f"æœ€å°å€¼ç‚¹: ({solution.x[0]:.2f}, {solution.x[1]:.2f})")
print(f"æœ€å°å€¼: {solution.fun:.4f}")
```
### å‡¸ä¼˜åŒ–åŸºç¡€ï¼šä¸ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜çš„ç‰¹ä¾‹
```
graph LR
    A[ä¼˜åŒ–é—®é¢˜] --> B{æ˜¯å¦ä¸ºå‡¸ï¼Ÿ}
    B -- æ˜¯ --> C[åªæœ‰ä¸€ä¸ªå…¨å±€æœ€ä¼˜è§£]
    B -- å¦ --> D[å¯èƒ½æœ‰å¤šä¸ªå±€éƒ¨æœ€ä¼˜è§£]
    
    subgraph å‡¸å‡½æ•°ç‰¹æ€§
    C --> E[äºŒé˜¶å¯¼æ•°>=0]
    C --> F[ä»»æ„è¿çº¿ä½äºå‡½æ•°ä¸Šæ–¹]
    C --> G[å±€éƒ¨æœ€ä¼˜å³å…¨å±€æœ€ä¼˜]
    end
```
å‡¸ä¼˜åŒ–çš„é»„é‡‘å®šå¾‹ï¼š
1. å‡¸é—®é¢˜æ€»èƒ½æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£  
2. æœºå™¨å­¦ä¹ ä¸­å¸¸å°†éå‡¸é—®é¢˜è½¬åŒ–ä¸ºå‡¸é—®é¢˜æ±‚è§£
### å­¦ä¹ ç‡ç­–ç•¥ï¼šæ™ºèƒ½è°ƒæ•´å­¦ä¹ æ­¥ä¼
ä¸åŒå­¦ä¹ ç‡ç­–ç•¥å¯¹æ¯”ï¼š
```python
# ä¸‰ç§å­¦ä¹ ç‡ç­–ç•¥
def constant_lr(epoch):  # å›ºå®šå­¦ä¹ ç‡
    return 0.1

def step_lr(epoch):     # é˜¶æ¢¯ä¸‹é™
    if epoch < 10:
        return 0.1
    elif epoch < 20:
        return 0.01
    else:
        return 0.001

def exp_lr(epoch):      # æŒ‡æ•°è¡°å‡
    return 0.1 * (0.9 ** epoch)

# ç»˜åˆ¶å­¦ä¹ ç‡å˜åŒ–æ›²çº¿
epochs = range(1, 31)

plt.plot(epochs, [constant_lr(e) for e in epochs], 'b-o', label='å›ºå®šå­¦ä¹ ç‡')
plt.plot(epochs, [step_lr(e) for e in epochs], 'r-s', label='é˜¶æ¢¯è¡°å‡')
plt.plot(epochs, [exp_lr(e) for e in epochs], 'g-^', label='æŒ‡æ•°è¡°å‡')
plt.xlabel('è®­ç»ƒè½®æ¬¡(epoch)')
plt.ylabel('å­¦ä¹ ç‡')
plt.title('ä¸åŒå­¦ä¹ ç‡ç­–ç•¥æ¯”è¾ƒ')
plt.legend()
plt.grid(True)
plt.show()
```
## æ•°å­¦åœ¨AIä¸­çš„å®é™…åº”ç”¨
**å…¸å‹AIä»»åŠ¡ä¸­æ¶‰åŠçš„æ•°å­¦ï¼š**
| AIæ¨¡å‹       | çº¿æ€§ä»£æ•° | æ¦‚ç‡ç»Ÿè®¡ | å¾®ç§¯åˆ† | ä¼˜åŒ–æ–¹æ³• |
|--------------|----------|----------|--------|----------|
| çº¿æ€§å›å½’     | â˜…â˜…       | â˜…â˜…       | â˜…      | â˜…â˜…       |
| ç¥ç»ç½‘ç»œ     | â˜…â˜…â˜…      | â˜…        | â˜…â˜…â˜…    | â˜…â˜…â˜…      |
| æ¨èç³»ç»Ÿ     | â˜…â˜…       | â˜…â˜…â˜…      | â˜…      | â˜…â˜…       |
| å›¾åƒå¤„ç†     | â˜…â˜…â˜…      | â˜…        | â˜…      | â˜…â˜…       |
| å¼ºåŒ–å­¦ä¹      | â˜…        | â˜…â˜…â˜…      | â˜…â˜…     | â˜…â˜…â˜…      |
## å­¦ä¹ å»ºè®®ï¼š
â€‹â€‹1. ç†è§£ > è®°å¿†â€‹â€‹ï¼šå…ˆææ‡‚æ¦‚å¿µï¼Œå…¬å¼è‡ªç„¶è®°ä½  
â€‹â€‹2. å¯è§†åŒ–æ˜¯åˆ©å™¨â€‹â€‹ï¼šå¤šç”»å›¾å¸®åŠ©ç†è§£æŠ½è±¡æ¦‚å¿µ  
3. â€‹â€‹åŠ¨æ‰‹è®¡ç®—â€‹â€‹ï¼šPythonå·¥å…·åŒ…æ˜¯æ•°å­¦å­¦ä¹ å¥½å¸®æ‰‹  
4. â€‹â€‹å®é™…åº”ç”¨é©±åŠ¨â€‹â€‹ï¼šå…³æ³¨çŸ¥è¯†åœ¨AIä¸­çš„å…·ä½“ç”¨é€”  

é€šè¿‡è¿™ä»½æ•™ç¨‹ï¼Œæ‚¨å·²ç»åˆæ­¥æŒæ¡äº†AIæ‰€éœ€çš„æ•°å­¦åŸºç¡€ã€‚æ•°å­¦å°±åƒç¼–ç¨‹çš„"å†…åŠŸ"ï¼Œéœ€è¦æŒç»­ç»ƒä¹ æ‰èƒ½çœŸæ­£ç†è§£å…¶ç²¾é«“ï¼



# ğŸ¯ é˜¶æ®µ 1ï¼š[æœºå™¨å­¦ä¹ ï¼šé›¶åŸºç¡€å…¥é—¨æŒ‡å—](https://github.com/0voice/learning-Journey-AI/tree/main/Machine%20Learning)

> ***ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ***
>> æƒ³è±¡ä¸€ä¸‹ï¼Œä½ åœ¨æ•™å­©å­åŒºåˆ†çŒ«å’Œç‹—ï¼šä¸æ˜¯ç›´æ¥å‘Šè¯‰ä»–è§„åˆ™ï¼Œè€Œæ˜¯ç»™ä»–çœ‹å„ç§çŒ«ç‹—å›¾ç‰‡ï¼Œè®©ä»–è‡ªå·±æ€»ç»“ç‰¹å¾ã€‚  
>> **è¿™å°±æ˜¯æœºå™¨å­¦ä¹ ï¼è®©è®¡ç®—æœºé€šè¿‡å¤§é‡æ•°æ®è‡ªå·±å‘ç°è§„å¾‹ã€‚**
  
**æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹æ€»ç»“**
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
graph TD
    A[ç†è§£é—®é¢˜] --> B[æ•°æ®æ”¶é›†]
    B --> C[æ•°æ®é¢„å¤„ç†]
    C --> D[æ¨¡å‹é€‰æ‹©]
    D --> E[æ¨¡å‹è®­ç»ƒ]
    E --> F[æ¨¡å‹è¯„ä¼°]
    F --> G{æ•ˆæœæ»¡æ„?}
    G -->|å¦| D
    G -->|æ˜¯| H[è¶…å‚æ•°è°ƒä¼˜]
    H --> I[æœ€ç»ˆæ¨¡å‹è¯„ä¼°]
    I --> J[æ¨¡å‹éƒ¨ç½²]
```

**æ¥ä¸‹æ¥æˆ‘ä»¬ä»ä»¥ä¸‹å‡ ä¸ªç‚¹å¼€å§‹è®²è§£**  

- **ç›‘ç£å­¦ä¹ **  
  çº¿æ€§/é€»è¾‘å›å½’ Â· SVM Â· å†³ç­–æ ‘ Â· é›†æˆæ–¹æ³•
- **æ— ç›‘ç£å­¦ä¹ **  
  èšç±»(K-means, DBSCAN) Â· é™ç»´(PCA, t-SNE)
- **æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–**  
  äº¤å‰éªŒè¯ Â· è¶…å‚æ•°è°ƒä¼˜ Â· è¯„ä¼°æŒ‡æ ‡

  
## ç›‘ç£å­¦ä¹ ï¼šæœ‰è€å¸ˆçš„æŒ‡å¯¼å­¦ä¹ 
### 1. çº¿æ€§å›å½’ï¼šé¢„æµ‹è¿ç»­å€¼
- â€‹â€‹æ ¸å¿ƒæ€æƒ³â€‹â€‹ï¼šæ‰¾åˆ°ä¸€æ¡æœ€ä½³æ‹Ÿåˆçº¿ï¼Œé¢„æµ‹è¿ç»­å€¼ç»“æœ
- å®ä¾‹åº”ç”¨â€‹â€‹ï¼šæ ¹æ®æˆ¿å­é¢ç§¯é¢„æµ‹æˆ¿ä»·
```python
# ç®€å•çº¿æ€§å›å½’ç¤ºä¾‹
import numpy as np
from sklearn.linear_model import LinearRegression

# æˆ¿å­é¢ç§¯æ•°æ®ï¼ˆå¹³æ–¹ç±³ï¼‰
house_sizes = np.array([50, 70, 90, 110]).reshape(-1, 1)
# å¯¹åº”æˆ¿ä»·ï¼ˆä¸‡å…ƒï¼‰
prices = np.array([300, 400, 500, 600])

# åˆ›å»ºæ¨¡å‹å¹¶è®­ç»ƒ
model = LinearRegression()
model.fit(house_sizes, prices)

# é¢„æµ‹120å¹³æˆ¿å­çš„ä»·æ ¼
prediction = model.predict([[120]])
print(f"é¢„æµ‹æˆ¿ä»·: {prediction[0]:.1f}ä¸‡å…ƒ")  # è¾“å‡º: é¢„æµ‹æˆ¿ä»·: 700.0ä¸‡å…ƒ
```

### 2. é€»è¾‘å›å½’ï¼šè§£å†³äºŒåˆ†ç±»é—®é¢˜  

- â€‹â€‹æ ¸å¿ƒæ€æƒ³â€‹â€‹ï¼šè®¡ç®—æŸä»¶äº‹å‘ç”Ÿçš„æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰  
- â€‹â€‹å®ä¾‹åº”ç”¨â€‹â€‹ï¼šåˆ¤æ–­é‚®ä»¶æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶
```python
# åƒåœ¾é‚®ä»¶è¯†åˆ«ç¤ºä¾‹
from sklearn.linear_model import LogisticRegression

# å‡è®¾æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š
# feature1: é‚®ä»¶åŒ…å«"å…è´¹"æ¬¡æ•°
# feature2: é‚®ä»¶åŒ…å«"è·å¥–"æ¬¡æ•°
X_train = [[3, 1], [5, 2], [1, 0], [0, 1]]  # è®­ç»ƒæ•°æ®
y_train = [1, 1, 0, 0]  # 1=åƒåœ¾é‚®ä»¶ï¼Œ0=æ­£å¸¸é‚®ä»¶

# åˆ›å»ºæ¨¡å‹å¹¶è®­ç»ƒ
spam_detector = LogisticRegression()
spam_detector.fit(X_train, y_train)

# é¢„æµ‹æ–°é‚®ä»¶
new_email = [[4, 3]]  # åŒ…å«4æ¬¡"å…è´¹"ï¼Œ3æ¬¡"è·å¥–"
prediction = spam_detector.predict(new_email)
print("åƒåœ¾é‚®ä»¶" if prediction[0] == 1 else "æ­£å¸¸é‚®ä»¶")  # è¾“å‡º: åƒåœ¾é‚®ä»¶
```

### 3. æ”¯æŒå‘é‡æœº(SVM)ï¼šæ‰¾æœ€ä½³å†³ç­–è¾¹ç•Œ  
- æ ¸å¿ƒæ€æƒ³â€‹â€‹ï¼šåœ¨æ•°æ®ç‚¹ä¹‹é—´æ‰¾åˆ°æœ€å®½çš„"éš”ç¦»å¸¦"â€‹â€‹
- å®ä¾‹åº”ç”¨â€‹â€‹ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«  
 åŸºç¡€æ¦‚å¿µå›¾ç¤ºï¼š
```mermaid
graph TD
    A[æ”¯æŒå‘é‡æœºæ ¸å¿ƒæ€æƒ³] --> B[å¯»æ‰¾æœ€ä½³è¶…å¹³é¢]
    A --> C[æœ€å¤§åŒ–åˆ†ç±»é—´éš”]
    A --> D[å¤„ç†éçº¿æ€§æ•°æ®]
    style A fill:#f9f,stroke:#333
```

### 4. å†³ç­–æ ‘ï¼šæ ‘çŠ¶å†³ç­–æ¨¡å‹
- â€‹â€‹å·¥ä½œåŸç†â€‹â€‹ï¼šåƒ"20ä¸ªé—®é¢˜"æ¸¸æˆï¼Œé€šè¿‡ä¸€ç³»åˆ—é—®é¢˜å¾—å‡ºç»“è®º
â€‹â€‹- å®ä¾‹åº”ç”¨â€‹â€‹ï¼šè´·æ¬¾å®¡æ‰¹å†³ç­–
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
graph TD
    A[æ”¶å…¥>2ä¸‡ï¼Ÿ] -->|æ˜¯| B[ä¿¡ç”¨è¯„åˆ†>700ï¼Ÿ]
    A -->|å¦| C[æ‹’ç»è´·æ¬¾]
    B -->|æ˜¯| D[æ‰¹å‡†è´·æ¬¾]
    B -->|å¦| E[æä¾›æŠµæŠ¼ï¼Ÿ]
    E -->|æ˜¯| D
    E -->|å¦| C
```

### 5. é›†æˆæ–¹æ³•ï¼šå›¢ç»“åŠ›é‡å¤§
ä¸‰ç§å¸¸ç”¨æ–¹æ³•ï¼š
| æ–¹æ³•         | å·¥ä½œåŸç†                     | ä¼˜ç‚¹                  |
|--------------|----------------------------|-----------------------|
| éšæœºæ£®æ—     | å¤šæ£µæ ‘å…±åŒæŠ•ç¥¨               | æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›å¼º        |
| æ¢¯åº¦æå‡æ ‘   | åä¸€æ£µæ ‘ä¿®æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯    | é¢„æµ‹ç²¾åº¦é«˜            |
| AdaBoost     | é‡ç‚¹è®­ç»ƒéš¾åˆ†ç±»æ ·æœ¬           | å¤„ç†ä¸å¹³è¡¡æ•°æ®        |

## æ— ç›‘ç£å­¦ä¹ ï¼šæ— äººæŒ‡å¯¼çš„è‡ªæˆ‘å‘ç°
### 1. èšç±»åˆ†æï¼šç‰©ä»¥ç±»èš
**K-meansèšç±»**  
- â€‹â€‹å·¥ä½œåŸç†â€‹â€‹ï¼šè‡ªåŠ¨å°†æ•°æ®åˆ†æˆKä¸ªç°‡
- â€‹â€‹å®ä¾‹åº”ç”¨â€‹â€‹ï¼šå¸‚åœºç»†åˆ†åˆ†æ
```python
# å®¢æˆ·åˆ†ç¾¤ç¤ºä¾‹
from sklearn.cluster import KMeans
import numpy as np

# å‡è®¾æœ‰ä¸¤ç§å®¢æˆ·ç‰¹å¾ï¼šè´­ä¹°é¢‘ç‡å’Œå¹³å‡å®¢å•ä»·
customer_data = np.array([
    [1, 100],   # å®¢æˆ·1
    [5, 500],   # å®¢æˆ·2
    [1, 150],   # å®¢æˆ·3
    [6, 550]    # å®¢æˆ·4
])

# åˆ›å»ºK=2çš„èšç±»æ¨¡å‹
kmeans = KMeans(n_clusters=2)
kmeans.fit(customer_data)

# æŸ¥çœ‹åˆ†ç¾¤ç»“æœ
print("å®¢æˆ·åˆ†ç¾¤ç»“æœ:", kmeans.labels_)
# å¯èƒ½è¾“å‡º: [0, 1, 0, 1] è¡¨ç¤ºåˆ†æˆä¸¤ç»„
```
**K-meanså¯è§†åŒ–è¿‡ç¨‹ï¼š**
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
graph LR
    A[éšæœºé€‰æ‹©ä¸­å¿ƒç‚¹] --> B[å°†ç‚¹åˆ†é…åˆ°æœ€è¿‘ä¸­å¿ƒ]
    B --> C[é‡æ–°è®¡ç®—ä¸­å¿ƒç‚¹ä½ç½®]
    C --> D{ä¸­å¿ƒç‚¹å˜åŒ–?}
    D -->|æ˜¯| B
    D -->|å¦| E[è¾“å‡ºèšç±»ç»“æœ]
```
**DBSCANèšç±»**
- â€‹â€‹ç‰¹ç‚¹â€‹â€‹ï¼šè‡ªåŠ¨å‘ç°ä»»æ„å½¢çŠ¶çš„èšç±»ç°‡
â€‹â€‹- é€‚ç”¨åœºæ™¯â€‹â€‹ï¼šåœ°ç†æ•°æ®èšç±»

### 2. é™ç»´æŠ€æœ¯ï¼šåŒ–ç¹ä¸ºç®€
**ä¸»æˆåˆ†åˆ†æ(PCA)**  
- â€‹â€‹å·¥ä½œåŸç†â€‹â€‹ï¼šå°†é«˜ç»´æ•°æ®å‹ç¼©åˆ°å…³é”®ç»´åº¦
- â€‹â€‹å®ä¾‹åº”ç”¨â€‹â€‹ï¼šäººè„¸è¯†åˆ«ç‰¹å¾æå–
```python
# PCAé™ç»´ç¤ºä¾‹
from sklearn.decomposition import PCA
import numpy as np

# åˆ›å»ºä¸€äº›ä¸‰ç»´æ•°æ®
data = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9],
    [10, 11, 12]
])

# åˆ›å»ºPCAæ¨¡å‹ï¼Œé™åˆ°äºŒç»´
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(data)

print("é™ç»´åæ•°æ®:")
print(reduced_data)
```
**t-SNEæŠ€æœ¯**
-â€‹â€‹ ç‰¹ç‚¹â€‹â€‹ï¼šä¿æŒç›¸ä¼¼ç‚¹å½¼æ­¤æ¥è¿‘
â€‹â€‹- é€‚ç”¨åœºæ™¯â€‹â€‹ï¼šé«˜ç»´æ•°æ®å¯è§†åŒ–ï¼ˆå¦‚MNISTæ‰‹å†™æ•°å­—ï¼‰

## æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–
### 1. è¯„ä¼°æŒ‡æ ‡ï¼šè€ƒå·è¯„åˆ†
**å›å½’é—®é¢˜æŒ‡æ ‡**
| æŒ‡æ ‡   | å…¬å¼                               | ç‰¹ç‚¹                     |
|--------|-----------------------------------|--------------------------|
| MAE    | \( \frac{1}{n}\sum_{i=1}^{n} \|y_i - \hat{y_i}\| \) | é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¹³å‡ç»å¯¹è¯¯å·® |
| MSE    | \( \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2 \) | å¯¹å¤§è¯¯å·®æƒ©ç½šæ›´å¤§           |
| RÂ²     | \( 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} \) | è¡¨ç¤ºæ¨¡å‹è§£é‡ŠåŠ›           |

**åˆ†ç±»é—®é¢˜æŒ‡æ ‡**
| æŒ‡æ ‡     | è®¡ç®—å…¬å¼                                | é€‚ç”¨åœºæ™¯             |
|----------|----------------------------------------|----------------------|
| å‡†ç¡®ç‡   | \( \frac{TP + TN}{TP + FP + FN + TN} \) | å‡è¡¡æ•°æ®             |
| ç²¾ç¡®ç‡   | \( \frac{TP}{TP + FP} \)               | æ³¨é‡é¢„æµ‹è´¨é‡         |
| å¬å›ç‡   | \( \frac{TP}{TP + FN} \)               | æ³¨é‡æŸ¥å…¨ç‡           |
| F1åˆ†æ•°   | \( 2 \times \frac{Precision \times Recall}{Precision + Recall} \) | ç»¼åˆæŒ‡æ ‡             |

### 2. äº¤å‰éªŒè¯ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
**â€‹â€‹ä¼ ç»ŸéªŒè¯ vs KæŠ˜äº¤å‰éªŒè¯**
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
graph LR
    A[æ•°æ®] --> B[è®­ç»ƒé›†]
    A --> C[æµ‹è¯•é›†]
    
    D[æ•°æ®] --> E[æŠ˜1:æµ‹è¯•é›†]
    D --> F[æŠ˜2:æµ‹è¯•é›†]
    D --> G[æŠ˜3:æµ‹è¯•é›†]
    D --> H[...]
    
    style A fill:#f9f,stroke:#333
    style B fill:#ccf,stroke:#333
    style C fill:#fcc,stroke:#333
    style D fill:#f9f,stroke:#333
    style E fill:#fcc,stroke:#333
    style F fill:#fcc,stroke:#333
    style G fill:#fcc,stroke:#333
```
```python
# äº¤å‰éªŒè¯ç¤ºä¾‹
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

# åˆ›å»ºæ¨¡å‹
model = RandomForestClassifier()

# ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯
scores = cross_val_score(model, X, y, cv=5)

print(f"äº¤å‰éªŒè¯å¾—åˆ†: {scores}")
print(f"å¹³å‡å‡†ç¡®ç‡: {scores.mean():.2f}")
```
### 3. è¶…å‚æ•°è°ƒä¼˜ï¼šæ¨¡å‹å¾®è°ƒ
â€‹â€‹**ä¸¤ç§ä¸»è¦æ–¹æ³•â€‹â€‹ï¼š**
1. **ç½‘æ ¼æœç´¢**â€‹â€‹ï¼šå°è¯•æ‰€æœ‰å¯èƒ½çš„å‚æ•°ç»„åˆ
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7]
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print("æœ€ä½³å‚æ•°ç»„åˆ:", grid_search.best_params_)
```
2. â€‹**éšæœºæœç´¢**â€‹â€‹ï¼šéšæœºé‡‡æ ·å‚æ•°ç»„åˆï¼Œæ›´é«˜æ•ˆ
```python
from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    'n_estimators': range(50, 500, 50),
    'max_depth': range(3, 15)
}

random_search = RandomizedSearchCV(RandomForestClassifier(), 
                                 param_dist, 
                                 n_iter=20, 
                                 cv=5)
random_search.fit(X_train, y_train)
```

> **è®°ä½ï¼šæœºå™¨å­¦ä¹ ä¸æ˜¯é­”æ³•ï¼å¥½çš„æ¨¡å‹ = 70%æ•°æ®è´¨é‡ + 20%ç‰¹å¾å·¥ç¨‹ + 10%æ¨¡å‹é€‰æ‹©ä¸è°ƒä¼˜**   
>> å¼€å§‹ä½ çš„æœºå™¨å­¦ä¹ ä¹‹æ—…å§ï¼å®è·µæ˜¯æœ€å¥½çš„å­¦ä¹ æ–¹æ³•ï¼Œå°è¯•è§£å†³Kaggleä¸Šçš„å…¥é—¨ç«èµ›æ¥ç§¯ç´¯ç»éªŒã€‚

**ğŸ“˜ æ¨èèµ„æºï¼š**
- [Andrew Ng æœºå™¨å­¦ä¹ è¯¾ç¨‹](https://www.coursera.org/learn/machine-learning)
- [ğŸ“– ã€Šæœºå™¨å­¦ä¹ ã€‹ - å‘¨å¿—å](https://book.douban.com/subject/26708119/)

# ğŸ”¥ é˜¶æ®µ 2ï¼š[æ·±åº¦å­¦ä¹ ](https://github.com/0voice/learning-Journey-AI/tree/main/Deep%20learning)
> æ·±åº¦å­¦ä¹ å…¥é—¨æŒ‡å— ğŸš€
>> æ·±åº¦å­¦ä¹ å°±åƒæ•™å©´å„¿è®¤è¯†ä¸–ç•Œâ€‹â€‹ï¼šå…ˆè®¤è¯†å½¢çŠ¶ï¼ˆåŸºç¡€ç†è®ºï¼‰ï¼Œå†è®¤äººè„¸ï¼ˆè®¡ç®—æœºè§†è§‰ï¼‰ï¼Œ  
>> ç„¶åå­¦è¯´è¯ï¼ˆNLPï¼‰ï¼Œæœ€åå­¦ä¼šåˆ›ä½œï¼ˆç”Ÿæˆæ¨¡å‹ï¼‰ã€‚ä¸‹é¢å¸¦ä½ çœ‹æ‡‚è¿™ä¸ªç¥å¥‡ä¸–ç•ŒğŸ‘‡

| æ–¹å‘         | æ ¸å¿ƒæŠ€æœ¯                        | å­¦ä¹ èµ„æº                             |
|--------------|---------------------------------|--------------------------------------|
| åŸºç¡€ç†è®º     | ç¥ç»ç½‘ç»œÂ·åå‘ä¼ æ’­Â·æ­£åˆ™åŒ–        | [æ·±åº¦å­¦ä¹ ](https://www.deeplearningbook.org/) |
| è®¡ç®—æœºè§†è§‰   | CNNÂ·ç›®æ ‡æ£€æµ‹Â·å›¾åƒåˆ†å‰²           | [CS231n](http://cs231n.stanford.edu/)         |
| NLP          | RNNã€Transformerã€BERTã€LLMs          | [NLPè¯¾ç¨‹](https://course.fast.ai/)  |
| ç”Ÿæˆæ¨¡å‹     | GANã€Diffusionã€ChatGPT              | [Hugging Face](https://huggingface.co/)       |



## [ç¥ç»ç½‘ç»œè¶…è¯¦ç»†å›¾è§£ï¼šå°ç™½çš„3Dæ‹†è§£æŒ‡å— ğŸ§ ](Deep%20learning/ç¥ç»ç½‘ç»œ.md)
æƒ³è±¡ç¥ç»ç½‘ç»œå°±åƒä¸€å¥—ä¹é«˜ç§¯æœ¨å·¥å‚ï¼è¾“å…¥æ˜¯åŸæ–™ï¼Œè¾“å‡ºæ˜¯æˆå“ï¼Œéšè—å±‚å°±æ˜¯å±‚å±‚ç»„è£…æµæ°´çº¿ã€‚ä¸‹é¢å¸¦ä½ èµ°è¿›è¿™ä¸ªç¥å¥‡å·¥å‚ï¼š

### ä¸€ã€æ ¸å¿ƒç»“æ„ï¼šä¸‰å±‚æµæ°´çº¿ç³»ç»Ÿ
**æ ¸å¿ƒä¸‰ä»¶å¥—â€‹â€‹ï¼š**
1. ç¥ç»ç½‘ç»œâ€‹â€‹
- åƒäººè„‘ç¥ç»å…ƒç½‘ç»œï¼šè¾“å…¥å±‚ï¼ˆçœ¼ç›çœ‹ï¼‰â†’ éšè—å±‚ï¼ˆå¤§è„‘æ€è€ƒï¼‰â†’ è¾“å‡ºå±‚ï¼ˆå˜´å·´è¯´ï¼‰
- å¯è§†åŒ–ç†è§£ï¼š
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
graph LR
    A[è¾“å…¥æ•°æ®] --> B[ç¥ç»å…ƒ1]
    A --> C[ç¥ç»å…ƒ2]
    B --> D[è¾“å‡ºç»“æœ]
    C --> D
    style A fill:#9f9
    style B fill:#f99
    style C fill:#f99
    style D fill:#99f
```

```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
graph LR
    A[åŸæ–™ä»“åº“] -->|è¾“å…¥æ•°æ®| B[é›¶ä»¶åŠ å·¥çº¿]
    B --> C[ç²¾å¯†ç»„è£…çº¿]
    C --> D[è´¨æ£€åŒ…è£…çº¿]
    D --> E[æˆå“ä»“åº“]
    
    style A fill:#9f9,stroke:#333
    style B fill:#f99,stroke:#333
    style C fill:#f99,stroke:#333
    style D fill:#f99,stroke:#333
    style E fill:#99f,stroke:#333
```
- è¾“å…¥å±‚â€‹â€‹ â†’ åŸæ–™ä»“åº“ï¼ˆæ¥æ”¶åŸå§‹æ•°æ®ï¼šå¦‚å›¾åƒåƒç´ /æ–‡å­—ç¼–ç ï¼‰
- â€‹â€‹éšè—å±‚â€‹â€‹ â†’ ç»„è£…è½¦é—´ï¼ˆå¤šå±‚æµæ°´çº¿å¤„ç†ç‰¹å¾ï¼‰
- â€‹â€‹è¾“å‡ºå±‚â€‹â€‹ â†’ æˆå“ä»“åº“ï¼ˆç”Ÿæˆç»“æœï¼šå¦‚"çŒ«/ç‹—"åˆ†ç±»ï¼‰

2. åå‘ä¼ æ’­â€‹â€‹
- å­¦ä¹ è¿‡ç¨‹ï¼šè€ƒè¯•åè€å¸ˆæ‰¹æ”¹è¯•å· â†’ å‘Šè¯‰ä½ å“ªé‡Œé”™äº† â†’ ä¸‹æ¬¡æ”¹è¿›
- æ•°å­¦æœ¬è´¨ï¼šä»è¾“å‡ºå±‚å€’æ¨è°ƒæ•´æ¯ä¸ªç¥ç»å…ƒçš„"é‡è¦æ€§æƒé‡"  

3. æ­£åˆ™åŒ–â€‹â€‹  
- é˜²"æ­»è®°ç¡¬èƒŒ"ï¼šç»™å­¦ç”Ÿåˆ’é‡ç‚¹ï¼ˆé™ä½å¤æ‚åº¦ï¼‰ï¼Œé¿å…è€ƒè¯•æ¢é¢˜å°±æŒ‚ç§‘ï¼ˆè¿‡æ‹Ÿåˆï¼‰
- å¸¸ç”¨æ–¹æ³•ï¼šDropoutï¼ˆéšæœºå±è”½ç¥ç»å…ƒï¼‰ã€L1/L2ï¼ˆæ§åˆ¶æƒé‡æ•°å€¼ï¼‰

#### ğŸ“Œ â€‹â€‹çœŸå®æ¡ˆä¾‹â€‹â€‹ï¼šäººè„¸è¯†åˆ«ç³»ç»Ÿ
- è¾“å…¥å±‚ï¼šæ¥æ”¶128x128åƒç´ å›¾ç‰‡ï¼ˆ=16,384ä¸ªè¾“å…¥ç‚¹ï¼‰
- éšè—å±‚ï¼šå±‚å±‚æå–çœ¼ç›/é¼»å­ç­‰ç‰¹å¾
- è¾“å‡ºå±‚ï¼šåˆ¤æ–­è¿™æ˜¯å¦æ˜¯ç‰¹å®šäººç‰©

### äºŒã€ç¥ç»å…ƒï¼šå·¥å‚é‡Œçš„æ™ºèƒ½æœºå™¨äºº
æ¯ä¸ªç¥ç»å…ƒéƒ½æ˜¯å¾®å‹è®¡ç®—å•å…ƒï¼š
```python
# å•ä¸ªç¥ç»å…ƒçš„å·¥ä½œä»£ç 
def ç¥ç»å…ƒ(è¾“å…¥ä¿¡å·, æƒé‡, åç½®):
    weighted_sum = sum(è¾“å…¥ä¿¡å· * æƒé‡) + åç½®  # åŠ æƒæ±‚å’Œ
    return æ¿€æ´»å‡½æ•°(weighted_sum)           # éçº¿æ€§è½¬æ¢
```
- æƒé‡(weight)â€‹â€‹ â†’ å·¥äººç»éªŒå€¼ï¼ˆè€å·¥äººæ›´å…³æ³¨å…³é”®ç‰¹å¾ï¼‰
- â€‹â€‹åç½®(bias)â€‹â€‹ â†’ è´¨æ£€æ ‡å‡†ï¼ˆè°ƒæ•´åˆ¤æ–­æ¾ç´§åº¦ï¼‰
- â€‹â€‹æ¿€æ´»å‡½æ•°â€‹â€‹ â†’ æ ¸å¿ƒï¼è®©æœºå™¨å…·å¤‡"æ€è€ƒèƒ½åŠ›"çš„ç§˜å¯†æ­¦å™¨

**å¸¸è§æ¿€æ´»å‡½æ•°å¯¹æ¯”ï¼š**
  
| å‡½æ•°åç§°   | å·¥ä½œæ–¹å¼               | é€‚ç”¨åœºæ™¯       | å½¢è±¡æ¯”å–»         |
|------------|-----------------------|---------------|------------------|
| Sigmoid    | å‹ç¼©åˆ°0-1åŒºé—´         | æ¦‚ç‡é¢„æµ‹       | æ¸©å’Œçš„è€å¸ˆå‚…     |
| ReLU       | è´Ÿæ•°å½’é›¶ï¼Œæ­£æ•°ä¿ç•™     | 90%ç°ä»£ç½‘ç»œ    | æœæ–­çš„è´¨æ£€å‘˜ âœ…  |
| Tanh       | å‹ç¼©åˆ°-1åˆ°1åŒºé—´       | RNNç½‘ç»œ       | ä¸¥æ ¼çš„å·¥ç¨‹å¸ˆ     |

ğŸ”¥ ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ  
æ²¡æœ‰å®ƒ â†’ ç¥ç»ç½‘ç»œåªæ˜¯é«˜çº§è®¡ç®—å™¨ï¼ˆåªèƒ½å¤„ç†çº¿æ€§é—®é¢˜ï¼‰  
åŠ ä¸Šå®ƒ â†’ ç¥ç»ç½‘ç»œå˜èº«ä¸‡èƒ½è¿‘ä¼¼å™¨ï¼ˆå¯å¤„ç†ä»»æ„å¤æ‚é—®é¢˜ï¼‰  

### ä¸‰ã€è®­ç»ƒè¿‡ç¨‹ï¼šå·¥å‚å¸ˆå¾’æ•™å­¦ç³»ç»Ÿ
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
sequenceDiagram
    Master->>Apprentice: Give 1000 cat and dog images
    %% å¸ˆå‚…ç»™å­¦å¾’1000å¼ çŒ«ç‹—å›¾ç‰‡
    Apprentice->>Master: First prediction (accuracy 40%)
    %% å­¦å¾’ç¬¬ä¸€æ¬¡é¢„æµ‹(å‡†ç¡®ç‡40%)
    Master->>Apprentice: Calculate prediction error (loss function)
    %% å¸ˆå‚…è®¡ç®—é¢„æµ‹é”™è¯¯ç¨‹åº¦(æŸå¤±å‡½æ•°)
    Master->>Apprentice: Provide correction method (backpropagation)
    %% å¸ˆå‚…åå‘æŒ‡å¯¼ä¿®æ­£æ–¹æ³•(åå‘ä¼ æ’­)
    loop Repeated practice
        Apprentice->>Master: New prediction
        %% å­¦å¾’æ–°ä¸€æ¬¡é¢„æµ‹
        Master->>Apprentice: Correct weight parameters
        %% å¸ˆå‚…ä¿®æ­£æƒé‡å‚æ•°
    end
    Apprentice->>Master: Final prediction (accuracy 95%) ğŸ‘
    %% å­¦å¾’æœ€ç»ˆé¢„æµ‹(å‡†ç¡®ç‡95%) ğŸ‘
```
#### å…³é”®è®­ç»ƒç»„ä»¶ï¼š
1. â€‹â€‹æŸå¤±å‡½æ•°â€‹â€‹ â†’ æˆç»©å•
- åˆ†ç±»ä»»åŠ¡ï¼šäº¤å‰ç†µï¼ˆCross-Entropyï¼‰
 æŸå¤± = -Î£(çœŸå®å€¼ * log(é¢„æµ‹å€¼))
- å›å½’ä»»åŠ¡ï¼šå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰
 æŸå¤± = Î£(é¢„æµ‹å€¼ - çœŸå®å€¼)Â² / n
â€‹â€‹2. ä¼˜åŒ–å™¨â€‹â€‹ â†’ æ•™å­¦æ–¹æ³•
- åŸºç¡€ç‰ˆï¼šæ¢¯åº¦ä¸‹é™
 æ–°æƒé‡ = æ—§æƒé‡ - å­¦ä¹ ç‡ Ã— æ¢¯åº¦
- æ™ºèƒ½ç‰ˆï¼šAdamä¼˜åŒ–å™¨ï¼ˆè‡ªåŠ¨è°ƒèŠ‚å­¦ä¹ ç‡ï¼‰
3. â€‹â€‹åå‘ä¼ æ’­â€‹â€‹ â†’ é”™é¢˜åˆ†æ
- ä»è¾“å‡ºå±‚å¼€å§‹é€å±‚å›æº¯
- ç”¨é“¾å¼æ³•åˆ™è®¡ç®—å„å±‚æƒé‡éœ€è°ƒæ•´çš„ç¨‹åº¦

> ğŸ’¡ å­¦ä¹ ç‡å°è´´å£«ï¼š  
> å¤ªå¤§ â†’ å­¦å¾’æµ®èºä¹±æ”¹å‚æ•°ï¼ˆéœ‡è¡ä¸æ”¶æ•›ï¼‰  
> å¤ªå° â†’ å­¦å¾’è¿›æ­¥ç¼“æ…¢ï¼ˆè®­ç»ƒé€Ÿåº¦æ…¢ï¼‰  
> ç†æƒ³å€¼ â†’ 0.001åˆ°0.1ä¹‹é—´ï¼ˆéœ€å®éªŒè°ƒæ•´ï¼‰  

### å››ã€å®æˆ˜æ¼”ç¤ºï¼šæ‰‹å†™æ•°å­—è¯†åˆ«
ç”¨Python+Kerasæ­å»º28x28åƒç´ è¯†åˆ«ç½‘ç»œï¼š
```python
from keras.models import Sequential
from keras.layers import Dense

# æ­å»ºæµæ°´çº¿
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(784,))) # é¦–å±‚éœ€æŒ‡å®šè¾“å…¥å°ºå¯¸
model.add(Dense(256, activation='relu'))     # éšè—å±‚2
model.add(Dense(128, activation='relu'))     # éšè—å±‚3
model.add(Dense(10, activation='softmax'))   # è¾“å‡ºå±‚(10ä¸ªæ•°å­—æ¦‚ç‡)

# é…ç½®ç”Ÿäº§çº¿
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# å¼€å§‹è®­ç»ƒ(ä½¿ç”¨MNISTæ•°æ®é›†)
model.fit(x_train, y_train, epochs=10)

# æµ‹è¯•æ•ˆæœ
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"è¯†åˆ«å‡†ç¡®ç‡: {test_acc*100:.1f}%")  # å…¸å‹ç»“æœï¼š98.2%
```
**ç½‘ç»œç»“æ„å¯è§†åŒ–ï¼š**
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
flowchart TD
    A[è¾“å…¥å±‚ 784èŠ‚ç‚¹] --> B[éšè—å±‚1 512èŠ‚ç‚¹]
    B --> C[éšè—å±‚2 256èŠ‚ç‚¹]
    C --> D[éšè—å±‚3 128èŠ‚ç‚¹]
    D --> E[è¾“å‡ºå±‚ 10èŠ‚ç‚¹]
    
    style A fill:#9f9
    style B fill:#f99
    style C fill:#f99
    style D fill:#f99
    style E fill:#99f
```

### äº”ã€ç¥ç»ç½‘ç»œç±»å‹å›¾è°±
| ç±»å‹             | ç»“æ„ç‰¹ç‚¹               | å…¸å‹åº”ç”¨          |
|------------------|------------------------|-------------------|
| å…¨è¿æ¥ç½‘ç»œ       | æ¯å±‚ç¥ç»å…ƒå…¨éƒ¨äº’è”      | åŸºç¡€åˆ†ç±»/å›å½’      |
| CNN              | å·ç§¯å±‚+æ± åŒ–å±‚ç»„åˆ       | å›¾åƒå¤„ç† âœ…        |
| RNN              | å¸¦æ—¶é—´å¾ªç¯è¿æ¥          | æ–‡æœ¬/è¯­è¨€         |
| Transformer      | è‡ªæ³¨æ„åŠ›æœºåˆ¶            | NLPä»»åŠ¡ âœ…        |


**ğŸš€ å‡çº§æŠ€å·§ï¼š**

- æ·»åŠ Dropoutå±‚ï¼šéšæœºåœå·¥éƒ¨åˆ†æµæ°´çº¿ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
- æ‰¹æ ‡å‡†åŒ–(BatchNorm)ï¼šç»Ÿä¸€é›¶ä»¶è§„æ ¼ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰
- è¿ç§»å­¦ä¹ ï¼šç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„è€å¸ˆå‚…ï¼ˆå¦‚ResNet/VGGï¼‰

> ç¥ç»ç½‘ç»œå°±åƒä¹é«˜å·¥å‚â€”â€”é€šè¿‡ç®€å•çš„é›¶ä»¶ï¼ˆç¥ç»å…ƒï¼‰ç»„åˆï¼Œæœ€ç»ˆèƒ½å»ºé€ å‡ºæ™ºèƒ½å¸å›½å¤§å¦ï¼  
> ç°åœ¨å°±åœ¨Google ColabåŠ¨æ‰‹æ­å»ºä½ çš„ç¬¬ä¸€ä¸ªç½‘ç»œå§ï¼

### ğŸ“š â€‹â€‹å­¦ä¹ èµ„æºâ€‹â€‹ï¼š
ã€Š[æ·±åº¦å­¦ä¹ ](https://github.com/exacity/deeplearningbook-chinese/tree/master)ã€‹(èŠ±ä¹¦) - AIé¢†åŸŸçš„"åœ£ç»"ï¼Œé…å¥¶èŒ¶æ…¢æ…¢å•ƒæ•ˆæœæ›´ä½³ â˜•

## äºŒã€è®¡ç®—æœºè§†è§‰ï¼šæœºå™¨çš„"çœ¼ç›" 0.0
### æ ¸å¿ƒæŠ€æœ¯ç»„åˆæ‹³â€‹â€‹ï¼š
1. CNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰â€‹â€‹
**å·¥ä½œåŸç†ï¼šåƒç”¨æ”¾å¤§é•œåˆ†å±‚æ‰«æå›¾ç‰‡**
- å·ç§¯å±‚ï¼šè¯†åˆ«å±€éƒ¨ç‰¹å¾ï¼ˆå¦‚çŒ«è€³æœµã€è½¦è½®ï¼‰
- æ± åŒ–å±‚ï¼šå‹ç¼©å…³é”®ä¿¡æ¯ï¼ˆå»é™¤éé‡ç‚¹èƒŒæ™¯ï¼‰
- å…¨è¿æ¥å±‚ï¼šç»¼åˆåˆ¤æ–­ï¼ˆæ‹¼æ¥ç‰¹å¾å¾—å‡ºç»“è®ºï¼‰
2. â€‹â€‹ç›®æ ‡æ£€æµ‹â€‹â€‹
- ç»å…¸æ¨¡å‹ï¼šYOLOï¼ˆYou Only Look Onceï¼‰

 **å®æ—¶æ£€æµ‹æ•ˆæœâ€‹â€‹ï¼š**
```
è¾“å…¥ï¼šè¡—é“å›¾ç‰‡ â†’ è¾“å‡ºï¼š  
[æ±½è½¦ï¼šåæ ‡(x1,y1)  ç½®ä¿¡åº¦98%]  
[è¡Œäººï¼šåæ ‡(x2,y2)  ç½®ä¿¡åº¦92%]
```
3. â€‹å›¾åƒåˆ†å‰²â€‹â€‹
- åŒ»ç–—åº”ç”¨ï¼šCTç‰‡ä¸­è‡ªåŠ¨æ ‡å‡ºè‚¿ç˜¤åŒºåŸŸï¼ˆåƒç´ çº§è¯†åˆ«ï¼‰

### ğŸ¥ â€‹â€‹å­¦ä¹ èµ„æºâ€‹â€‹ï¼š
[æ–¯å¦ç¦CS231nè¯¾ç¨‹](https://www.bilibili.com/video/BV1nJ411z7fe/)ï¼ˆBç«™æœ‰ä¸­æ–‡ç‰ˆï¼‰â†’ çœ‹5èŠ‚è¯¾å°±èƒ½è‡ªå·±å†™å›¾åƒè¯†åˆ«ç¨‹åºï¼

## ä¸‰ã€NLPï¼šè®©æœºå™¨æ‡‚äººè¯ ğŸ’¬
### â€‹â€‹å…³é”®æŠ€æœ¯æ¼”è¿›â€‹â€‹ï¼š
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
timeline
    title NLPæŠ€æœ¯å‘å±•å²
    2015å¹´ ï¼š RNNï¼ˆè®°å¿†çŸ­ï¼Œå¤„ç†é•¿æ–‡æœ¬åƒåŠ›ï¼‰
    2017å¹´ ï¼š Transformerï¼ˆæ³¨æ„åŠ›æœºåˆ¶çªç ´ï¼‰
    2018å¹´ ï¼š BERTï¼ˆåŒå‘ç†è§£ä¸Šä¸‹æ–‡ï¼‰
    2023å¹´ ï¼š ChatGPTï¼ˆå¯¹è¯èƒ½åŠ›çˆ†å‘ï¼‰
```
1. RNN/Transformerâ€‹â€‹
- RNNé—®é¢˜ï¼š"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨"å­¦åˆ°"å¤©å®‰é—¨"æ—¶å·²å¿˜è®°å¼€å¤´
- Transformeré©æ–°ï¼šåŒæ—¶å…³æ³¨æ‰€æœ‰è¯ â†’ ç†è§£"è‹¹æœ"åœ¨æ°´æœ/æ‰‹æœºä¸­çš„ä¸åŒå«ä¹‰
2. â€‹â€‹è¯åµŒå…¥(Word Embedding)â€‹â€‹
- æŠŠè¯è¯­å˜æˆæ•°å­—å¯†ç 
```
å›½ç‹ - ç”·äºº + å¥³äºº = å¥³ç‹
vec(å·´é») - vec(æ³•å›½) + vec(æ—¥æœ¬) â‰ˆ vec(ä¸œäº¬)
```

### ğŸ“ƒ â€‹â€‹å­¦ä¹ èµ„æºâ€‹â€‹ï¼š
BERTè®ºæ–‡ç²¾è¯» + Hugging Faceå®æˆ˜ â†’ 3å¤©æ­å»ºè‡ªå·±çš„æ–‡æœ¬æƒ…æ„Ÿåˆ†æå™¨

## å››ã€ç”Ÿæˆæ¨¡å‹ï¼šæœºå™¨çš„"æƒ³è±¡åŠ›" ğŸ¨  
### ä¸‰å¤§åˆ›ä½œå¼•æ“â€‹â€‹ï¼š
| æŠ€æœ¯        | ä»£è¡¨ä½œ       | åˆ›ä½œèƒ½åŠ›                     |
|-------------|-------------|-----------------------------|
| GAN         | äººè„¸ç”Ÿæˆ     | å›¾åƒç”Ÿæˆ/æ¢è„¸                |
| Diffusion   | DALLE 2     | æ–‡ç”Ÿå›¾ï¼ˆè¾“å…¥"æ˜Ÿç©ºä¸‹çš„ç†ŠçŒ«"å‡ºå›¾ï¼‰ |
| LLMs        | ChatGPT     | å†™è¯—/ç¼–ç /èŠäººç”Ÿ             |

**GANå·¥ä½œåŸç†â€‹â€‹ï¼š**
```mermaid
%% é“¾è¡¨/æ ‘/å›¾ - ä½¿ç”¨Mermaidç»˜åˆ¶
graph LR
    A[ç”Ÿæˆå™¨] -->|ä¼ªé€ åç”»| B[é‰´åˆ«å™¨]
    B -->|é‰´åˆ«çœŸä¼ª| A
    style A fill:#f9f
    style B fill:#9ff
```
- ç”Ÿæˆå™¨ï¼šåƒé€ å‡å¸çš„å›¢é˜Ÿ
- é‰´åˆ«å™¨ï¼šåƒé“¶è¡ŒéªŒé’æœº
- åŒæ–¹å¯¹æŠ—æå‡ï¼Œç›´åˆ°å‡å¸æ— æ³•è¢«è¯†åˆ«

### ğŸ¤– â€‹â€‹å­¦ä¹ èµ„æºâ€‹â€‹ï¼š
[Hugging Faceå¹³å°](https://huggingface.co/)ï¼ˆAIç•ŒGitHubï¼‰â†’ ç›´æ¥åœ¨çº¿ä½“éªŒStable Diffusionç”Ÿæˆå›¾ç‰‡ï¼

> **ğŸ’¡ â€‹â€‹å…³é”®æç¤º**â€‹â€‹ï¼šæ·±åº¦å­¦ä¹ â‰ é­”æ³•ï¼å…ˆæŒæ¡åŸºç¡€ç†è®ºå†æ”»å…·ä½“æ–¹å‘ï¼Œé‡åˆ°å…¬å¼åˆ«æ€•â†’å…ˆè·‘é€šä»£ç å†å›å¤´ç†è§£ç†è®ºæ•ˆæœæ›´ä½³ï¼


### ğŸ¯ é˜¶æ®µ 3ï¼š[å·¥å…·ä¸å®è·µ](https://github.com/0voice/learning-Journey-AI/tree/main/tools)
- **æ¡†æ¶æŒæ¡**  
  PyTorch Â· TensorFlow Â· JAX
- **æ•°æ®å¤„ç†**  
  Pandas Â· NumPy Â· OpenCV Â· NLTK
- **æ¨¡å‹éƒ¨ç½²**  
  ONNX Â· TensorRT Â· Flask/Django
- **MLOpsåŸºç¡€**  
  MLflow Â· Weights & Biases Â· Docker


## ğŸ” å¿«é€Ÿå…¥å£

| æˆ‘æ˜¯...  | å¿«é€Ÿå…¥å£                                                         |
| ------ | ------------------------------------------------------------ |
| åˆå­¦è€…    | [ğŸ“˜ Python å¿«é€Ÿå…¥é—¨](https://github.com/0voice/learning-Journey-AI/tree/main/Python%20and%20Math) |
| æœ‰åŸºç¡€è€…   | [ğŸ“˜ æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ](https://github.com/0voice/learning-Journey-AI/tree/main/Machine%20Learning)                 |
| æƒ³ç›´æ¥åšé¡¹ç›® | [ğŸ”§ å®æˆ˜é¡¹ç›®é›†](https://github.com/pytorch/examples)                         |
| ç ”ç©¶çˆ±å¥½è€…  | [ğŸ“˜ è®ºæ–‡ç²¾è¯»æŒ‡å—](https://github.com/terryum/awesome-deep-learning-papers)           |

## ğŸš§ å®æˆ˜é¡¹ç›®ç¤ºä¾‹
### CNNå›¾åƒåˆ†ç±»ç¤ºä¾‹ - PyTorch

```python
# CNNå›¾åƒåˆ†ç±»ç¤ºä¾‹ - PyTorch
import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, datasets
```

### 1. åŠ è½½æ•°æ®é›†
```python
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)
```

### 2. æ„å»ºCNNæ¨¡å‹
```python
class CNNClassifier(nn.Module):
    def __init__(self):
        super(CNNClassifier, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64 * 8 * 8, 512)
        self.relu3 = nn.ReLU()
        self.fc2 = nn.Linear(512, 10)

    def forward(self, x):
        x = self.pool1(self.relu1(self.conv1(x)))
        x = self.pool2(self.relu2(self.conv2(x)))
        x = self.flatten(x)
        x = self.relu3(self.fc1(x))
        x = self.fc2(x)
        return x

model = CNNClassifier()
```

### 3. è®­ç»ƒæ¨¡å‹
```python
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    running_loss = 0.0
    for i, (images, labels) in enumerate(train_loader, 0):
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        
    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')

print('è®­ç»ƒå®Œæˆ!')
```

### ğŸ” æ›´å¤šå®Œæ•´é¡¹ç›®ï¼š

- [å›¾åƒåˆ†ç±»å®æˆ˜](https://github.com/0voice/learning-Journey-AI/blob/main/Project/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%AE%9E%E6%88%98.md)
- [æ–‡æœ¬æƒ…æ„Ÿåˆ†æ](https://github.com/0voice/learning-Journey-AI/blob/main/Project/%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.md)
- [èŠå¤©æœºå™¨äººæ„å»º](https://github.com/0voice/learning-Journey-AI/blob/main/Project/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%9E%84%E5%BB%BA.md)


## ğŸ“š æ ¸å¿ƒèµ„æºæ€»è§ˆ
### ğŸ¥åœ¨çº¿è¯¾ç¨‹æ¨è
- [Machine Learning - Andrew Ng (Coursera)](https://www.coursera.org/learn/machine-learning)

- [Deep Learning Specialization - deeplearning.ai](https://www.deeplearning.ai/program/deep-learning-specialization/)

- [Fast.ai Practical DL](https://course.fast.ai/)

### ğŸ“–ç”µå­ä¹¦ç±ç²¾é€‰
- [ã€Šæ·±åº¦å­¦ä¹ ã€‹ - Ian Goodfellow](https://www.deeplearningbook.org/)

- [ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹ - ææ²](https://zh.d2l.ai/)

- [ã€Šå¼ºåŒ–å­¦ä¹ å¯¼è®ºã€‹ - Sutton & Barto](http://incompleteideas.net/book/the-book-2nd.html)

- [ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ - æèˆª](https://github.com/SmirkCao/Lihang)

### ğŸ“°ç»å…¸è®ºæ–‡
- [Attention is All You Need](https://arxiv.org/abs/1706.03762)

- [ResNet (Deep Residual Learning)](https://arxiv.org/abs/1512.03385)

- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)

### ğŸ§°å¼€å‘å·¥å…·
- [Google Colab](https://colab.research.google.com/)

- [VS Code](https://code.visualstudio.com/)

- [Hugging Face Transformers](https://github.com/huggingface/transformers)

## ğŸ’– è‡´è°¢
- å¼€æºç¤¾åŒºæä¾›çš„ä¼˜ç§€å·¥å…·

- æ•™è‚²å…ˆé©±ï¼šç¤¾ä¼šå„ç•ŒAIå­¦è€…ï¼Œå´æ©è¾¾æ•™æˆç­‰æ•™è‚²å…ˆé©±

- æ‚¨ï¼æ¯ä¸€ä½ä½¿ç”¨å’Œä¼ æ’­èµ„æ–™çš„å­¦ä¹ è€…

> â€œäººå·¥æ™ºèƒ½å¦‚åŒæ–°æ—¶ä»£çš„ç”µåŠ›ï¼Œå°†é‡å¡‘æ‰€æœ‰è¡Œä¸šã€‚â€ â€” Andrew Ng  
> ğŸŒ± å¼€å¯ä½ çš„ AI å­¦ä¹ ä¹‹æ—…ï¼Œå°±ä»è¿™é‡Œå¼€å§‹ï¼
