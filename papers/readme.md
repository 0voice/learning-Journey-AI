# æœ‰å…³æ·±åº¦å­¦ä¹ çš„å­¦æœ¯è®ºæ–‡èµ„æº
***æ‰€æœ‰PDFæ–‡ä»¶è¯·ä¸‹è½½åæµè§ˆï¼ï¼ï¼***

## ğŸ”® é€šç”¨å¤§æ¨¡å‹ / åŸºç¡€æ¨¡å‹ï¼ˆFoundation Modelsï¼‰
| è®ºæ–‡       | é“¾æ¥                                                                                                                              | å…³é”®è¯            |
| -------- | ------------------------------------------------------------------------------------------------------------------------------- | -------------- |
| GPT-4    | [GPT-4 Technical Report (2023)](https://arxiv.org/abs/2303.08774)                                                               | å¤§è¯­è¨€æ¨¡å‹          |
| LLaMA 2  | [LLaMA 2: Open Foundation and Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)                                  | å¼€æºå¤§æ¨¡å‹          |
| Mistral  | [Mistral: Faster and Better (2023)](https://arxiv.org/abs/2310.06825)                                                           | é«˜æ•ˆ Transformer |
| Gemini   | [Gemini 1: Unlocking multimodal understanding (2023)](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) | å¤šæ¨¡æ€            |
| Claude 3 | [Anthropic Claude 3 Report (2024)](https://www.anthropic.com/index/claude-3)                                                    | å¤šæ¨¡æ€å¯¹è¯          |


## ğŸ§  è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰
| è®ºæ–‡          | é“¾æ¥                                                                                                                           | å…³é”®è¯             |
| ----------- | ---------------------------------------------------------------------------------------------------------------------------- | --------------- |
| BERT        | [BERT: Pre-training of Deep Bidirectional Transformers (2018)](https://arxiv.org/abs/1810.04805)                             | Transformer ç¼–ç å™¨ |
| T5          | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2020)](https://arxiv.org/abs/1910.10683) | æ–‡æœ¬ç»Ÿä¸€æ¡†æ¶          |
| InstructGPT | [Training language models to follow instructions (2022)](https://arxiv.org/abs/2203.02155)                                   | RLHF            |
| LoRA        | [LoRA: Low-Rank Adaptation of Large Language Models (2021)](https://arxiv.org/abs/2106.09685)                                | å¾®è°ƒæ–¹æ³•            |


## ğŸ§‘â€ğŸ¨ è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰
| è®ºæ–‡                 | é“¾æ¥                                                                                                | å…³é”®è¯     |
| ------------------ | ------------------------------------------------------------------------------------------------- | ------- |
| Vision Transformer | [An Image is Worth 16x16 Words (2020)](https://arxiv.org/abs/2010.11929)                          | ViT     |
| DINOv2             | [DINOv2 (2023)](https://arxiv.org/abs/2304.07193)                                                 | è‡ªç›‘ç£è§†è§‰æ¨¡å‹ |
| Segment Anything   | [Segment Anything (2023)](https://arxiv.org/abs/2304.02643)                                       | å›¾åƒåˆ†å‰²    |
| CLIP               | [CLIP: Learning Transferable Visual Models (2021)](https://arxiv.org/abs/2103.00020)              | å›¾æ–‡å¯¹é½    |
| DreamBooth         | [DreamBooth: Fine Tuning Text-to-Image Diffusion Models (2022)](https://arxiv.org/abs/2208.12242) | ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆ |

## ğŸ” å¤šæ¨¡æ€ï¼ˆå›¾æ–‡/è§†é¢‘/è¯­éŸ³ï¼‰
| è®ºæ–‡       | é“¾æ¥                                                                                              | å…³é”®è¯   |
| -------- | ----------------------------------------------------------------------------------------------- | ----- |
| Flamingo | [Flamingo: A Visual Language Model (2022)](https://arxiv.org/abs/2204.14198)                    | å›¾æ–‡é—®ç­”  |
| BLIP-2   | [BLIP-2: Bootstrapping Language-Image Pre-training (2023)](https://arxiv.org/abs/2301.12597)    | å›¾æ–‡ç”Ÿæˆ  |
| Whisper  | [Whisper: Robust Speech Recognition (2022)](https://openai.com/research/whisper)                | è¯­éŸ³è¯†åˆ«  |
| SpeechT5 | [SpeechT5: Unified-Modal Encoder-Decoder Pre-training (2022)](https://arxiv.org/abs/2110.07205) | å¤šè¯­ç§è¯­éŸ³ |

## ğŸ¤– å¼ºåŒ–å­¦ä¹  / è‡ªåŠ¨æ™ºèƒ½ä½“ï¼ˆAgentï¼‰
| è®ºæ–‡         | é“¾æ¥                                                                                             | å…³é”®è¯        |
| ---------- | ---------------------------------------------------------------------------------------------- | ---------- |
| AlphaGo    | [Mastering the game of Go (2016)](https://www.nature.com/articles/nature16961)                 | å¼ºåŒ–å­¦ä¹ ç»å…¸     |
| ReAct      | [ReAct: Synergizing Reasoning and Acting (2022)](https://arxiv.org/abs/2210.03629)             | è¯­è¨€æ¨¡å‹ Agent |
| AutoGPT    | [Auto-GPT (GitHub Repo)](https://github.com/Torantulino/Auto-GPT)                              | LLM è‡ªåŠ¨è§„åˆ’   |
| Voyager    | [Voyager: LLM-Powered Embodied Agent in Minecraft (2023)](https://voyager.minedojo.org/)       | æ¸¸æˆ AI      |
| OpenAgents | [OpenAgents: An Open Platform for LLM Agent Research (2024)](https://arxiv.org/abs/2404.13722) | å¤šä»»åŠ¡æ™ºèƒ½ä½“     |

## ğŸ“Š å‘é‡æ£€ç´¢ / å‘é‡æ•°æ®åº“ / Embedding
| è®ºæ–‡               | é“¾æ¥                                                                                     | å…³é”®è¯     |
| ---------------- | -------------------------------------------------------------------------------------- | ------- |
| FAISS            | [FAISS: Efficient Similarity Search (2017)](https://github.com/facebookresearch/faiss) | å‘é‡æ£€ç´¢åº“   |
| HNSW             | [HNSW: Efficient and Robust ANN (2018)](https://arxiv.org/abs/1603.09320)              | é«˜æ•ˆ ANN  |
| GTE              | [GTE: General Text Embeddings (2023)](https://huggingface.co/thenlper/gte-base)        | é€šç”¨æ–‡æœ¬å‘é‡  |
| OpenAI Embedding | [OpenAI Text Embeddings (2022)](https://platform.openai.com/docs/guides/embeddings)    | GPT å‘é‡åŒ– |



